# MySQL深入学习

## 一.影响MySQL性能的因素和顺序

影响大小的顺序的排列:

- 数据库的结构设计和sql语句的优化
- 存储引擎的选择和参数的配置
- 系统的选择和优化
- 硬件升级

### 1.硬件的影响

- CPU的频率和数量,尤其是web应用需要更高的并发量.因此对于MySQL来说,<u>CPU数量会更多地影响MySQL的性能,而在CPU密集型场景和复杂sql情况下,则是频率越高越好
- 内存,内存足够大,缓存中的数据可以更多的命中磁盘中的数据,就避免了对磁盘频繁的读写操作.
- 内存的大小,当内存足够大,缓存中的数据可以更多的命中磁盘中的数据,就避免了对磁盘频繁的读写操作.
- I/O子系统
- 网络

### 2.操作系统的影响

#### 1)对centos系统的参数优化

- 对内核有关的参数设置在etc/sysctl.conf文件下  要具体设置的话再去找找
- 资源增加限制etc/security/limit.conf文件下
- 磁盘调度策略 sys/block/devname/queue/schedule/文件下,
  - 电梯调度策略(noop)适合闪存设备,RAM,嵌入式设备
  - 截止时间调度策略(deadline)适合数据库
  - 预料I/O调度策略anticipatory 适合文件服务器

### 3.MySQL本身对于性能的影响

#### 1)各类存储引擎

1. MyISAM

它使用表级锁,在进行读写操作的时候会对表进行上锁

使用场景:

- 非事务型应用

- 只读性应用

  ---

    2.Innodb

在该存储引擎下,使用表空间进行数据存储

当innodb_file_per_table属性为on时,每个表有独立的表空间(强烈建议)

当innodb_file_per_table属性为off时,为每个表分配系统共享的表空间

- 它使用行级锁来进行资源管理,可以实现更高的并发
- 支持事务

---

3.CSV存储引擎

- 之前的数据存储方式是以二进制的方式存储,它用文本方式存储,因此可以直接对数据进行编辑
- 数据存储在csv为后缀的文件中
- 所有的列都是不能为null的
- 不支持索引,因此也就不支持大表和在线处理

使用场景:

- 适合作为数据交换的中间表(例如把excel文件数据放到csv文件,然后被其他web程序读取)

---

4.Archive

功能特点:

- 只支持insert和select操作
- 以zlib对表的数据进行压缩,占用磁盘资源更少
- 数据存储在ARZ为后缀的文件中
- 支持行级锁,可以进行高并发的插入和查询
- 只支持在自增ID列上增加索引

使用场景:

- 日志类和数据采集类的表(因为占用更小,但是如要进行修改,依然不适合)

---

5.Memory(HEAP)存储引擎

数据只保存在内存内,因此重启就没了

功能特点:

- 支持HASH(等值搜索)索引和BTree(范围搜索)索引
- 所有字段都为固定长度varchar(10)=char(10)
- 不支持BLOG TEXT等大字段
- 使用的是表级锁



Memory存储引擎表 和  临时表 的比较:

- 临时表是当前会话过程中为了方便而临时创建的,只对当前会话可见,而Memory表全局可见
- 临时表有两种
  - 系统创建的临时表如果没有超过限制,使用Memory引擎创建临时表,如果超出了使用Myisam引擎来创建.
  - 用户自己使用 create temporary table创建的临时表

使用场景:**由于数据容易丢失,所以要求数据可再生**

- 查找高效,所有可用于查找或者映射表,例如邮编和地点
- 用于保存数据分析中产生的中间表(注意和临时表区分)
- 用于缓存周期性聚合数据的结果表

---

6.Federated引擎(相当于远程表在本地的映射)

- MySQL本身没有提供访问远程服务器的表的功能,但是Federated引擎提供了
- 本地不存储数据,数据放到远程服务器上
- 但是本地还是需要保存表的结构和远程服务器的连接信息

使用方式:

这种引擎默认情况下是禁止的,首先要去MySQL的配置文件修改,添加federated=1 ( 具体操作可以去网上搜)

并且在创建本地数据库的时候除了指定引擎为Federated,还需要增加参数

```
connection=mysql://user_name[:password]@host_name[:post_num]/db_name/tbl_name
```

#### 2)引擎的选择

如今绝大部分情况下都采用Innodb引擎,但是在特殊情况下要采用其他引擎时,可以从如下几种情况考虑:

- 事务
- 备份
- 崩溃恢复
- 存储引擎的特有特性

### 3.MySQL服务器的参数设置

**MySQL读取配置文件的顺序**  由先到后

/etc/my.cnf -> /etc/mysql/my.cnf -> /home /mysql/my.cnf -> ~/.my.cnf

**为MySQL配置参数的作用域(不是在命令行参数设置,在mysql客户端设置):**

- 全局参数

set global 参数名=参数值

set @@global.参数名=参数值 

- 会话参数

set [session] 参数名=参数值

set @@session.参数名=参数值

**注意:具体的参数配置浏览器书签里有**



## 基准测试

### 容易忽视的注意点

- 使用生产环境的数据进行测试的时候,只使用部分数据,这会导致测试的不全面,降低测试的可靠性
- 在多用户的场景下,只做单用户的测试
- 在单服务器下,测试分布式应用,会导致测试结果比实际情况更好,无法起到优化的目的
- 反复执行相同的查询,容易缓存命中,无法反映真实的查询性能

### 常用的基准测试工具

mysqlslap  (MySQL自带,但不太推荐)

特点:

- 可以模拟服务器的负载,输出统计数据
- 可以指定或自动生成sql查询语句

**部分常用参数:**

--auto-generate-sql  表示由系统自动生成sql脚本继续测试

--auto-generate-sql-add-autoincrement:在生成的表中增加自增ID

--auto-generate-sql-load-type指定测试使用的语句类型

--auto-generate-sql-write-number指定初始化数据的生成的数据量

--concurrency 指定并发线程的数量

--engine 指定要测试表的存储引擎,用逗号分隔多种引擎

--no-drop指定不清理测试数据

--iterations指定测试次数  ( 和--no-drop相冲突)

--number-of-queries 指定每一个线程执行的查询数量

--debug-info 指定输出额外的内存和CPU统计信息

--number-int-cols指定测试表中包含的int类型的列的数量

--number-char-cols指定测试表中包含char类型的列的数量

--create-schema指定了用于执行测试的数据库的名字

--query用于指定自定义的sql脚本

--only-print 并不运行测试脚本,而是把生成的脚本打印出来

---

sysbench  

注意事项:

我们测试生成的数据量一定要比内存大,不然大部分数据被缓存就无法反映真实的查询情况

通过 free-m 可以获得内存大小

**书签里有**

## 数据库的表结构设计

### 各个数据类型的注意点

整型:

- int类型占用4个字节,因此,当只需要两位长度来存储时,即使写上int(2),依然占用4个长度
- tinyint占用一位,smallint占用2位,mediumint占用3位,bigint占用8位



varchar和char类型的比较:

- varchar类型的字符串长度是可变的,char类型的长度是定长
- varchar占用的存储长度与创建表时的长度无关,是实际长度,但是在执行语句的时候,占用内存依然是创建表时的长度,因此设计表的时候,依然要选择合适长度
- 在存储小长度的字符串时,还是用char更好一点
- 对于经常要更新的字符串,使用char更好一点

日期类型(**比字符串占用的存储空间更小,使用的时候,不要忘记日期函数解决一些问题**):

- datetime 占用8个字节,**没有时区的概念,在不同地区使用时间的话要注意**
- datetime默认没有微秒,如果是datetime(6)表示6位微秒
- timestamp占用4个字节,实际存储的是1970年1月1日带当前时间的秒数,但是用YYYY-MM-DD HH:MM:SS来显示,只能储存到2038年1月19号的时间
- timestamp有时区的概念,显示的是指定时区
- timestamp在行**被修改时可以自动修改timestamp列的值(默认只有第一个timestamp列可以有这个功能,但是哪一个列有这个功能时可以指定的),**因此可以用来表示最后修改时间
- MySQL5.7之后,新增date和time类型,占用3个字节,用来保存1000-1-1到9999-12-31的时间 YYYY-MM-DD
- time类型存储的是时分秒   HH:MM:SS



### 数据库的日志

#### 二进制日志

二进制日志:  存储了数据库中增删改查和对表所做的修改

5.7之前的MySQL版本,默认采用**段的存储方式,**直接存储了修改语句

```
binlog_format=statement
```

缺点:虽然完整记录了对数据库的修改语句,但是在使用它进行复制操作的时候,特定的函数如uuid()等不确定的函数,是不可以直接复制的,可能会导致主从数据库不一致的问题

---

5.7版本之后的MySQL默认采用**行的存储方式**

行的存储方式,存储了每一行具体的修改

```
binlog_format=ROW
```

优点:

- 复制时可以保证主从数据库的一致
- 对每一行的修改相比于段的更加高效

缺点:

- 记录的日志量更大,为了解决这个问题,因此MySQL提供了一个参数

  ```
  binlog_row_image=FULL(默认)/MINIMAL/NOBLOB
  FULL(默认):表示假如一个表20个列,一次修改时,修改了一个列,日志也会记录这20个列的前后值
  MINIMAL:表示假如一个表20个列,一次修改时,修改了一个列,日志记录这一个列的前后值
  NOBLOB:表示假如一个表20个列,一次修改时,修改了一个列,日志也会记录这20个列除了TEXT,BLOB类型的前后值
  ```

---

**混合日志格式:** 

根据sql语句,由系统决定用基于段还是 行的存储格式

数据量的大小由所执行的sql语句决定

```
binlog_format=MIXED
```

---

**三种格式对复制的影响**

段的存储格式:

- 完整记录了对数据库的修改语句,但是在使用它进行复制操作的时候,特定的函数如uuid()等不确定的函数,是不可以直接复制的,可能会导致主从数据库不一致的问题
- 相比于行的格式,在执行的时候要上更多的锁

行的存储格式:

- 要求主从服务器的数据库的表结构相同,否则可能中断复制
- 无法在从服务器上执行触发器,因为它没有执行sql语句,只是复制了主服务器的修改

### 数据库的复制

#### 工作流程:

主服务器把数据库的修改写入二进制日志->主服务器开启一个转储的线程

从服务器开启一个传输的线程连接主服务器的客户端,读取转储线程的数据,然后存入relay log中->

从服务器读取修改信息

#### 基于日志点的复制的配置步骤

1.在主DB服务器上建立一个复制账号,并授予权限

```
create user 'name'@'ip段' identified by 'password';
建议这个ip网段和存在从服务器的网段一致,防止被窃取二进制信息
grant replication slave on *.* to 'name'@'ip段';
```

2.配置主数据库服务器

bin_log=mysql-bin  //日志名字

server_id=  ___   //服务id,复制集群统一id

relay_log=___  //复制的日志名字,默认是主机的名字,如果有一天修改了可能会报错

log_slave_update=on(可选)   //是否要把这个复制日志的内容放到从服务器的二进制日志中,如果接下来要把这个从服务器作为主服务器来复制,这个选项一定要开启

read_only=on(可选)   //表示当前数据库的数据库只能被读取,不可写入



3.在从服务器的MySQL客户端上连接主服务器

------

优点:

1. bug相对较少
2. 对sql查询没有任何限制
3. 故障的处理比较容易

缺点:

1.故障转移时重新找到新的主库的日志点信息比较麻烦,尤其是一主多从

#### 基于GTID的复制

GTID是全局事务的ID,可以保证在主服务器上执行的事务在从服务器中可以生成一个唯一的ID

**操作步骤:**

1.在主DB服务器上建立一个复制账号,并授予权限

```
create user 'name'@'ip段' identified by 'password';
建议这个ip网段和存在从服务器的网段一致,防止被窃取二进制信息
grant replication slave on *.* to 'name'@'ip段';
```

2.配置主数据库服务器

bin_log=mysql-bin  //日志名字,建议和数据库的数据文件分开在不同的目录,最好是在不同的磁盘分区上,获得更好的IO性能

server_id=  ___   //

gtid_mode=on//使用gtid

enforce-gtid-consistency=   //强制事务的一致性,开启这个选项后,无法使用如下语句

```
create table...select

create  temporary  table
```

log-slave-updates=on

3.配置从服务器参数

server_id=  ___   //

relay_log=usr/local/mysql/log/relay_log  //复制的日志名字,默认是主机的名字,如果有一天修改了可能会报错

gtid_mode=on//使用gtid

enforce-gtid-consistency

log-slave-updates=on

以下为建议:

read_only=on

master_info_repository=TABLE

relay_log_info_repository=TABLE



3.在从服务器的MySQL客户端上连接主服务器

change master to master_host='master_host_ip',

​								master_user='repl',	

​								master_password='password',

​								master_auto_position=1	

---

优点:

方便故障转移;

缺点 ;

故障处理比较麻烦

### 数据库复制的性能优化

#### 影响主从延迟的因素:

- 主库写入二进制日志的时间  ->   要控制主库的事务大小,分割大事务

- 二进制从主库写入到从库,然后写入中继日志的时间  ->使用MINED的日志格式或设置或set binlog_row_image=minimal;

- 默认情况下从服务器上只有一个sql线程,主服务器上的并发修改在从服务器上编程的串行修改  -> 使用多线程复制

  ​	使用方法:

  1. 停止链路复制  stop slave
  2. set global slave_parallel_type=' logical_clock'//复制方式
  3. set global slave_parallel_worker=4 //线程数
  4. start slave

#### 常见的问题处理

1. 由于数据损坏或丢失引起的主从复制错误.(例如服务器宕机)

   主服务宕机可能会导致有些数据没有写入二进制日志

   从服务器宕机可能会导致重复执行一些事务

2. 主库的二进制日志损坏(比如服务器意外重启),会重新生成一个二进制文件,此时可能原来的二进制文件有些事务没有写入中继日志,因此要手动同步数据

3. 中继日志的损坏,这个比较简单,直接从主库复制就完事了

### 实现MySQL的高可用架构

#### 单点故障

定义:一个系统中提供相同功能的组件只有一个,一个故障了就会影响整个系统的运行

如何解决主服务器的单点问题:

- 主服务器切换后.怎么通知应用新的主服务器的IP地址

- 如何检查MySQL主服务器是否可用

- 如何处理从处理器和新的主服务器之间的复制关系

  #### MMM架构

  























## 三.SQL优化

### 1.定位慢查询

- 查看慢查询日志确定**已经执行完**的慢查询 
- show processlist 查看**正在执行**的慢查询

#### 1.1 通过慢查询定位

一般的方法是通过**慢查询日志**来查询的，MySQL 的慢查询日志用来记录在 MySQL 中响应时间超过参数 long_query_time（单位秒，默认值 10）设置的值并且扫描记录数不小于 min_examined_row_limit（默认值0）的语句，能够帮我们找到执行完的慢查询，方便我们对这些 SQL 进行优化

---

注意:

默认情况下，慢查询日志中不会记录管理语句，可通过设置 log_slow_admin_statements = on 让管理语句中的慢查询也会记录到慢查询日志中。 

默认情况下，也不会记录查询时间不超过 long_query_time ,但是不使用索引的语句，可通过配置 

log_queries_not_using_indexes = on 让不使用索引的 SQL 都被记录到慢查询日志中（即使查询时间没超过 long_query_time 配置的值）。 

----

#### 1.2查看慢查询日志的步骤

1. 开启慢查询日志    //**默认情况下是关闭的**

   ```
   mysql> set global slow_query_log = on;  
   Query OK, 0 rows affected (0.00 sec)
   ```

   

2. 设置慢查询阀值

   ```
   mysql> set global long_query_time = 1; 
   Query OK, 0 rows affected (0.00 sec)
   ```

   

3. 确定慢查询日志路径,慢查询日志的路径默认是 MySQL 的数据目录 

   ```
   show global variables like "datadir";
   ```

   

4. 确定慢查询 日志的文件名:

   ```
   show global variables like "slow_query_log_file";
   ```

   根据上面的查询结果，可以直接查看 /data/mysql/data/3306/mysql-slow.log 文件获取已经执行完的慢查询

示例:

![1595837699798](../../Desktop/assets/1595837699798.png)

上图参数讲解:

- tail -n5：只查看慢查询文件的最后5行 
- Time：慢查询发生的时间 
- User@Host：客户端用户和IP 
- Query_time：查询时间 
- Lock_time：等待表锁的时间 
- Rows_sent：语句返回的行数 
- Rows_examined：语句执行期间从存储引擎读取的行数 

**可以使用的工具:**

pt-query-digest 或者 mysqldumpslow

#### 1.3 查看当前正在进行的慢查询

```
show  full processlist;//show processlist 显示哪些线程正在运行。如果 有 PROCESS 权限，则可以看到所有线程。否则，只能看到当前会话的线程。
```

```
show  full processlist;//如果不使用 FULL 关键字，在 info 字段中只显示每个语句的前 100 个字符，如果想看语句的全部 内容可以使用 full 修饰
```



### 2. 分析慢查询

#### 2.1 使用explain分析慢查询(推荐)

作用:Explain 可以获取 MySQL 中 SQL 语句的执行计划，比如语句是否使用了关联查询、是否使用了索引、扫描行数等。可以帮我们选择更好地索引和写出更优的 SQL 。使用方法：在查询语句前面加上 explain 运行就可以了。

示例:

```
explain select * from t1 where b=100;
```

![1595838312328](../../Desktop/assets/1595838312328.png)



| 列名          | 解释                                                         |
| ------------- | ------------------------------------------------------------ |
| id            | 查询编号                                                     |
| select_type   | 查询类型：显示本行是简单还是复杂查询                         |
| table         | 涉及到的表                                                   |
| partitions    | 匹配的分区：查询将匹配记录所在的分区。仅当使用   partition 关键字时才显示该列。对于非分区表，该值为 NULL。 |
| type          | 本次查询的表连接类型                                         |
| possible_keys | 可能选择的索引                                               |
| key           | 实际选择的索引                                               |
| key_len       | 被选择的索引长度：一般用于判断联合索引有多少列被选择了       |
| ref           | 与索引比较的列                                               |
| rows          | 预计需要扫描的行数，对   InnoDB 来说，这个值是估值，并不一定准确 |
| filtered      | 按条件筛选的行的百分比                                       |
| Extra         | 附加信息                                                     |

##### 2.1.1各类参数详解

select_type

| select_type   的值   | 解释                                                        |
| -------------------- | ----------------------------------------------------------- |
| SIMPLE               | 简单查询(不使用关联查询或子查询)                            |
| PRIMARY              | 如果包含关联查询或者子查询，则最外层的查询部分标记为primary |
| UNION                | 联合查询中第二个及后面的查询                                |
| DEPENDENT UNION      | 满足依赖外部的关联查询中第二个及以后的查询                  |
| UNION RESULT         | 联合查询的结果                                              |
| SUBQUERY             | 子查询中的第一个查询                                        |
| DEPENDENT SUBQUERY   | 子查询中的第一个查询，并且依赖外部查询                      |
| DERIVED              | 用到派生表的查询                                            |
| MATERIALIZED         | 被物化的子查询                                              |
| UNCACHEABLE SUBQUERY | 一个子查询的结果不能被缓存，必须重新评估外层查询的每一行    |
| UNCACHEABLE UNION    | 关联查询第二个或后面的语句属于不可缓存的子查询              |

type,由上至下依次变差

| type的值        | 解释                                                         |
| --------------- | ------------------------------------------------------------ |
| system          | 查询对象表只有一行数据,且只能用于   MyISAM 和 Memory 引擎的表，这是最好的情况 |
| const           | 基于主键或唯一索引查询，最多返回一条结果                     |
| eq_ref          | 表连接时基于主键或非   NULL 的唯一索引完成扫描               |
| ref             | 基于普通索引的等值查询，或者表间等值连接                     |
| fulltext        | 全文检索                                                     |
| ref_or_null     | 表连接类型是   ref，但进行扫描的索引列中可能包含 NULL 值     |
| index_merge     | 利用多个索引                                                 |
| unique_subquery | 子查询中使用唯一索引                                         |
| index_subquery  | 子查询中使用普通索引                                         |
| range           | 利用索引进行范围查询                                         |
| index           | 全索引扫描                                                   |
| ALL             | 全表扫描                                                     |

Extra

| Extra   常见的值                      | 解释                                                         | 例子                                                         |
| ------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Using filesort                        | 将用外部排序而不是索引排序，数据较小时从内存排序，否则需要在磁盘完成排序 | explain   select * from t1 order by create_time;             |
| Using temporary                       | 需要创建一个临时表来存储结构，通常发生对没有索引的列进行   GROUP BY 时 | explain   select * from t1 group by create_time;             |
| Using index                           | 使用覆盖索引                                                 | explain   select a from t1 where a=111;                      |
| Using where                           | 使用   where 语句来处理结果                                  | explain   select * from t1 where create_time=‘2019-06-18 14:38:24’; |
| Impossible WHERE                      | 对   where 子句判断的结果总是 false 而不能选择任何数据       | explain   select * from t1 where 1<0;                        |
| Using join buffer (Block Nested Loop) | 关联查询中，被驱动表的关联字段没索引                         | explain   select * from t1 straight_join t2 on (t1.create_time=t2.create_time); |
| Using index condition                 | 先条件过滤索引，再查数据                                     | explain   select * from t1 where a >900 and a like “%9”;     |
| Select tables optimized away          | 使用某些聚合函数（比如   max、min）来访问存在索引的某个字段是 | explain   select max(a) from t1;                             |

#### 2.2 show profile分析慢循环

用途:可以通过配置参数 profiling = 1 来启用 SQL 分析。该参数可以在全局和 session 级别来设置。对于全局级别则作用于整个MySQL 实例，而 session 级别仅影响当前 session 。该参数开启后，后续**执行的 SQL 语句都将记录其资源开销，如 IO、上下文切换、CPU、Memory**等等。根据这些开销进一步分析当前 SQL 从而进行优化与调整。

1. 确定是否支持profile

```
select @@have_profiling;  如果得到的值为YES表示支持	profile
```

2. 查看profiling参数是否关闭

   ```
   select @@profiling;   如果得到的值为 0 ,表示没有开启
   ```

3. 查看profiling参数

   ```
   set (global) profiling=1;  开启(全局)当前会话的profiling 	   
   ```

4. 执行sql语句

   ```
   select * from t1 where b=1000;
   ```

5. 确定 SQL 的 query id

```
show profiles ;   通过 show profiles 语句确定执行过的 SQL 的 query id,假设此处得到的是1
```

6. 查询 SQL 执行详情

   ```
   show profile for query 1;  通过 show profile for query 可看到执行过的 SQL 每个状态和消耗时间：
   ```

示例:

![1595841052798](../../Desktop/assets/1595841052798.png)

#### 2.3 trace 分析 SQL 优化器

```
set session optimizer_trace="enabled=on",end_markers_in_json=on;//当前会话开启trace,并 JSON 输出开启结束标记 */
```

- rows：预计扫描的行数

- examined_rows：参与排序的行

- number_of_tmp_files：使用临时文件的个数

- sort_buffer_size：sort_buffer 的大小

- sort_mode：排序模式

  

   number_of_tmp_files 等于 7，所以表示使用的是磁盘排序。对于 number_of_tmp_files 等于 7 表示该 SQL 将需要排序的数据分为 7 份，然后每份单独排序，再存放在 7 个临时文件中，最后把 7 个临时文件合并成一个大的有序文件。

#### 2.4三种方式的适用用途

- explain:获取 MySQL 中 SQL 语句的执行计划，比如语句是否使用了关联查询、是否使用了索引、扫描行数等；
- profile：可以清楚了解到SQL到底慢在哪个环节；
- trace：查看优化器如何选择执行计划，获取每个可能的索引选择的代价。

### 3.有索引却无法使用导致查询慢

#### 3.1函数操作

首先,对条件字段做函数操作是无法走索引的,因此可以视情况把函数操作改写

#### 3.2隐式转换

当条件中的参数和字段实际类型不匹配时,就会进行隐式转换,,相当于使用了函数,因此同上.

```
a 字段类型是 varchar(20);
select * from t1 where a=1000;//命令
select * from t1 where cast(a as signed int) =1000;//实际命令
```

#### 3.3模糊查询

如下语句进行查询时也是全表查询:

```
explain select * from t1 where a like '%1111%';
```

但是如下语句是有索引的

```
select * from t1 where a like '1111%';//但是要根据情况而定,否则容易出错
```

#### 3.4  高数据量的范围查询

```
select * from t1 where b>=1 and b <=2000;//优化器会根据检索比例、表大小、I/O块大小等进行评估是否使用索引。比如单次查询的数据量过大，优化器将不走索引。
```

解决方法:

```
select * from t1 where b>=1 and b <=1000;
select * from t1 where b>=1001 and b <=2000;
把高数据量范围查询分成多次,就可以使用索引
```

**注意:实际这种范围查询而导致使用不了索引的场景经常出现，比如按照时间段抽取全量数据，每条SQL抽取一个月的；或者某张业务表历史数据的删除。遇到此类操作时，应该在执行之前对SQL做explain分析，确定能走索引，再进行操作，否则不但可能导致操作缓慢，在做更新或者删除时，甚至会导致表所有记录锁住，十分危险。**

#### 3.5  计算操作

```
select * from t1 where b-1 =1000;//对索引字段做运算将使用不了索引。
```

解决方案:

```
select * from t1 where b =1000 + 1;//将计算操作放在等号后面：
```

### 4.优化数据插入

#### 4.1 一条语句插入多条数据

一次插入多行花费时间0.2秒，一次插入一行花费了31秒，对比效果明显，因此建议**有大批量导入时，推荐一条insert语句插入多行数据。**



#### 4.2关闭自动提交 

Autocommit 开启时会为每个插入执行提交。可以在InnoDB导入数据时，关闭自动提交

```
SET autocommit=0;
INSERT INTO `t1` VALUES (1,'1',1,'2019-05-24 15:44:10');
INSERT INTO `t1` VALUES (2,'2',2,'2019-05-24 15:44:10');
INSERT INTO `t1` VALUES (3,'3',3,'2019-05-24 15:44:10');
......
COMMIT;
```

**结果:**

开启自动提交的情况下导入是31秒（执行详情见1.4 导入时间的对比）。

关闭自动提交的情况下导入是1秒左右，因此导入多条数据时，关闭自动提交，让多条 insert 一次提交，可以大大提升导入速度。

 **原因:**

与本节前面讲的一次插入多行能提高批量插入速度的原因一样，因为批量导入大部分时间耗费在客户端与服务端通信的时间，所以多条 insert 语句合并提交可以减少客户端与服务端通信的时间，并且合并提交还可以减少数据落盘的次数

#### 4.3 参数调整

影响MySQL写入速度的主要两个参数：**innodb_flush_log_at_trx_commit、sync_binlog。**

##### 4.3.1 参数解释

**innodb_flush_log_at_trx_commit：**控制重做日志刷新到磁盘的策略，有0 、1和2三种值。

- 0：master线程每秒把redo log buffer写到操作系统缓存，再刷到磁盘；
- 1：每次提交事务都将redo log buffer写到操作系统缓存，再刷到磁盘；
- 2：每次事务提交都将redo log buffer写到操作系统缓存，由操作系统来管理刷盘。

**sync_binlog：**控制binlog的刷盘时机，可配置0、1或者大于1的数字。

- 0：二进制日志从不同步到磁盘，依赖OS刷盘机制；
- 1：二进制日志每次提交都会刷盘；
- n(n>1) : 每n次提交落盘一次。

**bulk_insert_buffer_size**:批量插入的缓存

**key_buffer_size**:

**table_cache:**

结论:

经过sysbench测试后,innodb_flush_log_at_trx_commit设置为0、同时sync_binlog设置为0时，写入数据的速度是最快的。**如果对数据库安全性要求不高(比如你的测试环境)，可以尝试都设置为0后再导入数据，能大大提升导入速度**。

### 5.优化 查询效率

#### 5.1选取合适的字段属性

MySQL可以很好的支持大数据量的存取，一般说来，数据库中的表越小，执行的查询也就会越快。因此，为了获得更好的性能，我们可以将表中字段的宽度设得尽可能小。

例如，在定义邮政编码这个字段时，如果将其设置为CHAR(255),显然给数据库增加了不必要的空间，甚至使用VARCHAR这种类型也是多余的，因为CHAR(6)就可以很好的完成任务了。同样的，如果可以的话，我们应该使用MEDIUMINT而不是BIGIN来定义整型字段。

另外一个提高效率的方法是在可能的情况下，应该尽量把字段设置为NOTNULL，这样在将来执行查询的时候，数据库不用去比较NULL值。
对于某些文本字段，例如“省份”或者“性别”，我们可以将它们定义为ENUM类型。因为在MySQL中，ENUM类型被当作数值型数据来处理，而数值型数据被处理起来的速度要比文本类型快得多。这样，我们又可以提高数据库的性能。

#### 5.2 使用连接（JOIN）来代替子查询(Sub-Queries)

下面是子查询的语句

```mysql
SELECT * FROM customerinfo WHERE CustomerID NOT IN (SELECT customerID FROM salesinfo)
```

下面是使用join的语句:

```mysql
SELECT * FROM customerinfo LEFT JOIN salesinfo 
ON customerinfo.CustomerID=salesinfo.CustomerID 
WHERE salesinfo.CustomerID ISNULL
```

原因:  MySQL不需要在内存中创建临时表来完成这个逻辑上的需要两个步骤的查询工作

#### 5.3使用 UNION代替手动创建的临时表

它可以把需要使用临时表的两条或更多的select查询合并的一个查询中。在客户端的查询会话结束的时候，临时表会被自动删除，从而保证数据库整齐、高效。使用union来创建查询的时候，我们只需要用UNION作为关键字把多个select语句连接起来就可以了，要注意的是**所有select语句中的字段数目要想同。**

示例:

```mysql
SELECT Name,Phone FROM client UNION

SELECT Name,BirthDate FROM author UNION

SELECT Name,Supplier FROM product
```

#### 5.4保持数据库中数据的一致性和完整性:   事务

不是所有的数据库操作都可以只用一条或少数几条SQL语句就可以完成的。很多时候是需要用到一系列的语句来完成某种工作。设想一下，要把某个数据同时插入两个相关联的表中，可能会出现这样的情况：第一个表中成功更新后，数据库突然出现意外状况，造成第二个表中的操作没有完成，这样，就会造成数据的不完整，甚至会破坏数据库中的数据。要避免这种情况，就应该使用事务，它可以保持数据库中数据的一致性和完整性。事物以BEGIN关键字开始，COMMIT关键字结束。在这之间的一条SQL操作失败，那么，ROLLBACK命令就可以把数据库恢复到BEGIN开始之前的状态。

```mysql
BEGIN; 
INSERT INTO salesinfo SET CustomerID=14; 
UPDATE inventory SET Quantity=11 WHERE item='book'; 
COMMIT;
```

**另一个重要作用:**  

是当多个用户同时使用相同的数据源时，它可以利用锁定数据库的方法来为用户提供一种安全的访问方式，这样可以保证用户的操作不被其它的用户所干扰。

#### 5.5 锁定表

尽管事务是维护数据库完整性的一个非常好的方法，但却因为它的独占性，有时会影响数据库的性能，由于在事务执行的过程中，**数据库**将会被锁定，因此其它的用户请求只能暂时等待直到该事务结束。但假设有成千上万的用户同时访问一个数据库系统，例如访问一个电子商务网站，就会产生比较严重的响应延迟。

其实，有些情况下我们可以通过锁定表的方法来获得更好的性能。

示例:

```mysql
LOCK TABLE inventory WRITE ;
SELECT Quantity FROM inventory WHERE Item='book';
UPDATE inventory SET Quantity=11 WHERE Item='book'; 
UNLOCKTABLES
```



### 6. 优化sql语句的具体实行28条

1. 应尽量**避免在 where 子句中使用 != 或者 <> 操作符**，否则将引擎放弃使用索引而进行全表扫描。

2、应尽量**避免在 where 子句中对字段进行 null 值判断**，否则将导致引擎放弃使用索引而进行全表扫描，如：

```MySQL
select id from t where num is null;
```


可以在 num 上设置默认值 0 ，确保表中 num 列没有 null 值，然后这样查询：

```
select id from t where num = 0;
```

3、查询语句的**查询条件中只有OR关键字，并且OR前后的两个条件中的列都是索引**时，查询中才使用索引。否则，查询将不使用索引。

4、对查询进行优化，应尽量避免全表扫描，首先应**考虑在 where 及 order by 涉及的列上建立索引。**

5、前导模糊查询不能利用索引(like '%XX'或者like '%XX%')，可以使用索引覆盖避免。

6、**in 和 not in 也要慎用**，否则会导致全表扫描。如:

```mysql
select id from t where num in(1,2,3) 
```


对于连续的数值，**能用 between 就不要用 in 了：** 

```mysql
select id from t where num between 1 and 3 
```

7、如果**在 where 子句中使用参数，也会导致全表扫描**。因为 SQL 只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择到运行时；它必须在编译时进行选择。然而，如果在编译时简历访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描： 

```mysql
select id from t where num=@num
```


可以改为强制查询使用索引： 

```mysql
select id from t with(index(索引名)) where num=@num 
```

8、应尽量**避免在 where 子句中对字段进行表达式操作**，这将导致引擎放弃使用索引而进行全表扫描。如：

```mysql
select id from t where num/2 = 100;
```

应改为：

```mysql
select id from t where num = 100 * 2;
```

9、应尽量避免在 where 子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：

```mysql
select id from t where substring(name, 1, 3) = ’abc’–name;  //以abc开头的id
select id from t where datediff(day,createdate,’2005-11-30′) = 0–’2005-11-30′;  //生成的id
应改为:
select id from t where name like ‘abc%’
select id from t where createdate >= ’2005-11-30′ and createdate < ’2005-12-1′;
```


10、不要在 where 子句中的 “=” 左边进行函数，算术运算或者其他表达式运算，否则系统将可能无法正确使用索引。 

11、在使用索引字段作为条件时，**如果该索引是复合索引，那么必须使用到该索引的第一个字段作为条件时才能保证系统使用该索引，**否则该索引将不会被使用，并且应**尽可能的让字段顺序与索引顺序相一致。**

12、很多时候用 **exists 代替 in** 是一个好的选择： 

```mysql
select num from a where num in(select num from b);
```


用下面的语句替换： 

```mysql
select num from a where exists(select 1 from b where num=a.num);
```

13、**索引并不是越多越好**，索引固然可以提高相应的 select 的效率，但同时也**降低了 insert 及 update 的效率**，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。**一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。**

14、并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段 sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。

15、**尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，**这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会 逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。

16、**尽可能的使用 varchar/nvarchar 代替 char/nchar** ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。较一次就够了。

17、任何地方都不要使用 select * from t ，**用具体的字段列表代替 ***，不要返回用不到的任何字段。

18、尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。

19、避免频繁创建和删除临时表，以减少系统表资源的消耗。

20、临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。

21、在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先 create table，然后 insert。

22、如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。

23、尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。

24、使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。

25、与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时 间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。

26、在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。

27、尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。

28、尽量避免大事务操作，提高系统并发能力。

29. 如果要查询的字段都建立过索引，那么引擎会直接在索引表中查询而不会访问原始数据（否则只要有一个字段没有建立索引就会做全表扫描），这叫索引覆盖。因此我们需要尽可能的在`select`后只写必要的查询字段，以增加索引覆盖的几率。

    这里值得注意的是不要想着为每个字段建立索引，因为优先使用索引的优势就在于其体积小。

### 7.优化删除效率

删除数据的速度和创建的索引数量是成正比的。

- 所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟）
- 然后删除其中无用数据（此过程需要不到两分钟）
- 删除完成后重新创建索引(此时数据较少了)创建索引也非常快，约十分钟左右。
- 与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了。

### 8. 使用索引和创建索引的原则(还要看上面的29条和有索引无法使用的地方)

可以令数据库服务器以比没有索引快得多的速度检索特定的行，**尤其是在查询语句当中包含有MAX(),MIN()和ORDERBY这些命令的时候，性能提高更为明显。**

##### 8.1使用的地方:

一般说来，索引应建立在那些将用于JOIN,WHERE判断和ORDERBY排序的字段上。尽量不要对数据库中某个含有大量重复的值的字段建立索引。对于一个ENUM类型的字段来说，出现大量重复值是很有可能的情况,  在这样的字段上建立索引将不会有什么帮助；相反，还有可能降低数据库的性能。

此外，MySQL从版本3.23.23开始支持**全文索引**和搜索。全文索引在MySQL中是一个FULLTEXT类型索引，但仅能用于MyISAM类型的表。对于一个大的数据库，将数据装载到一个没有FULLTEXT索引的表中，然后再使用ALTERTABLE或CREATEINDEX创建索引，将是非常快的。但如果将数据装载到一个已经有FULLTEXT索引的表中，执行过程将会非常慢。

------

##### 8.2创建索引的原则（重中之重）

1） 最左前缀匹配原则，组合索引非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。

2）较频繁作为查询条件的字段才去创建索引

3）更新频繁字段不适合创建索引

4）若是不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低)

5）尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。

6）定义有外键的数据列一定要建立索引。

7）对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。

8）对于定义为text、image和bit的数据类型的列不要建立索引。

##### 8.3前缀索引

语法：`index(field(10))`，使用字段值的前10个字符建立索引，默认是使用字段的全部内容建立索引。

前提：前缀的标识度高。比如密码就适合建立前缀索引，因为密码几乎各不相同。

实操的难度：在于前缀截取的长度。

我们可以利用`select count(*)/count(distinct left(password,prefixLen));`，通过从调整`prefixLen`的值（从1自增）查看不同前缀长度的一个平均匹配度，接近1时就可以了（表示一个密码的前`prefixLen`个字符几乎能确定唯一一条记录）

##### 8.4 什么是最左前缀原则？什么是最左匹配原则

- 顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。
- 最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。
- =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式

### 9.提高order by,group by的查询速度

#### 9.1 Filesort 是在内存中还是在磁盘中完成排序的？

MySQL 中的 Filesort 并不一定是在磁盘文件中进行排序的，也有可能在内存中排序，内存排序还是磁盘排序取决于排序的数据大小和 sort_buffer_size 配置的大小。

- 如果 “排序的数据大小” < sort_buffer_size: 内存排序
- 如果 “排序的数据大小” > sort_buffer_size: 磁盘排序

#### 9.2 Filesort 下的排序模式

Filesort 下的排序模式有三种：

- < sort_key, rowid >双路排序（又叫回表排序模式）：是首先根据相应的条件取出相应的排序字段和可以直接定位行数据的行 ID，然后在 sort buffer 中进行排序，排序完后需要再次取回其它需要的字段；
- < sort_key, additional_fields >单路排序：是一次性取出满足条件行的所有字段，然后在sort buffer中进行排序；
- < sort_key, packed_additional_fields >打包数据排序模式：与单路排序相似，区别是将 char 和 varchar 字段存到 sort buffer 中时，更加紧缩。

因为打包数据排序模式是单路排序的一种升级模式，因此重点探讨双路排序和单路排序的区别。MySQL 通过比较系统变量 max_length_for_sort_data 的大小和需要查询的字段总大小来判断使用哪种排序模式。

- 如果 max_length_for_sort_data 比查询字段的总长度大，那么使用 < sort_key, additional_fields >排序模式；
- 如果 max_length_for_sort_data 比查询字段的总长度小，那么使用 <sort_key, rowid> 排序模式。

#### 9.3联合索引和order by的使用问题

首先  联合索引为(a,b)

```
select id,a,d from t1 where a=1000 order by d;
Extra 字段中看到 “Using filesort”，说明使用的是 filesort 排序。
select id,a,b from t1 where a=1000 order by b;
以在 Extra 字段中看到“Using index”，说明使用的是索引排序。
```

---

先等值过滤再排序的语句，可以通过在条件字段和排序字段添加联合索引来优化；但是如果**联合索引中前面的字段使用了范围查询，对后面的字段排序是否能用到索引排序**呢？下面我们通过实验验证一下：

```
select id,a,b from t1 where a>9000 order by b;
```

首先条件 a>9000 使用了索引（关注 key 字段对应的值为 idx_a_b）；在 Extra 中，看到了“Using filesort”，表示使用了 filesort 排序，并**没有使用索引排序**。所以联合索引中前面的字段使用了范围查询，对后面的字段排序使用不了索引排序。



#### 9.4修改和排序有关的参数

两个跟排序有关的参数：**max_length_for_sort_data、sort_buffer_size。**

- max_length_for_sort_data：如果觉得排序效率比较低，可以适当加大 max_length_for_sort_data 的值，让优化器优先选择全字段排序。当然不能设置过大，可能会导致 CPU 利用率过低或者磁盘 I/O 过高；
- sort_buffer_size：适当加大 sort_buffer_size 的值，尽可能让排序在内存中完成。但不能设置过大，可能导致数据库服务器 SWAP。

### 10.优化分页查询

#### 根据自增且连续主键排序的分页查询：

**使用条件:**

- **主键自增且连续**
- **结果是按照主键排序的**

首先来看一个根据自增且连续主键排序的分页查询的例子：

```mysql
select * from t1 limit 99000,2;
```

这条语句实际上查询了99002条语句,然后舍弃了前面99000条.  默认按照主键排序

key 字段为 NULL，表示未走索引

```mysql
select * from t1 where id >99000 limit 2;
```

改写后的 SQL key 字段为 PRIMARY，表示走了主键索引，扫描了1000行。

---

#### 根据非主键字段排序的分页查询

首先看一条语句:

```mysql
select * from t1 order by a limit 99000,2;
```

但是注意:

语句没有使用 a 字段的索引（key 字段对应的值为 null）：**扫描整个索引并查找到没索引的行的成本比扫描全表的成本更高，所以优化器放弃使用索引**。

优化途径:

关键是**让排序时返回的字段尽可能少**，所以可以让排序和分页操作先查出主键，然后根据主键查到对应的记录

```mysql
select * from t1 f inner join (select id from t1 order by a limit 99000,2)g on f.id = g.id;
```

需要的结果与要求一致，执行时间 0.02 秒，是原 SQL 执行时间的四分之一，原 SQL 使用的是 filesort 排序，而优化后的 SQL 使用的是索引排序。

## 五.事务的隔离级别

### 5.1 什么是脏读？幻读？不可重复读？

- 脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。
- 不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。
- 幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。

### 5.2 事物的隔离级别

为了达到事务的四大特性，数据库定义了4种不同的事务隔离级别，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏读、不可重复读、幻读这几类问题。

| 隔离级别         | 脏读 | 不可重复读 | 幻读 |
| ---------------- | ---- | ---------- | ---- |
| READ-UNCOMMITTED | √    | √          | √    |
| READ-COMMITTED   | ×    | √          | √    |
| REPEATABLE-READ  | ×    | ×          | √    |
| SERIALIZABLE     | ×    | ×          | ×    |

- **READ-UNCOMMITTED(读取未提交)：** 最低的隔离级别，允许读取尚未提交的数据变更，**可能会导致脏读、幻读或不可重复读**。
- **READ-COMMITTED(读取已提交)：** 允许读取并发事务已经提交的数据，**可以阻止脏读，但是幻读或不可重复读仍有可能发生**。
- **REPEATABLE-READ(可重复读)：** 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，**可以阻止脏读和不可重复读，但幻读仍有可能发生**。
- **SERIALIZABLE(可串行化)：** 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，**该级别可以防止脏读、不可重复读以及幻读**。

**注意:Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别**

因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是**READ-COMMITTED(读取提交内容):**，但是你要知道的是InnoDB 存储引擎默认使用 **REPEATABLE-READ（可重读）**并不会有任何性能损失。

InnoDB 存储引擎在 **分布式事务** 的情况下一般会用到**SERIALIZABLE(可串行化)**隔离级别。





## 六.分库分表

### 6.1 垂直切分和水平切分的优缺点

#### 6.1.1垂直切分

垂直切分是根据业务,把表切分到不同的库中

**优点：**
• 拆分后业务清晰，拆分规则明确；
• 系统之间整合或扩展容易；
• 数据维护简单。
**缺点：**
• 部分业务表无法 join，只能通过接口方式解决，提高了系统复杂度；
• 受每种业务不同的限制存在单库性能瓶颈，不易数据扩展跟性能提高；
• 事务处理复杂。

#### 6.1.2 水平切分

水平切分:按照某个字段的某种规则来分散到多个库之中，每个表中包含一部分数据。

---

**常用的切分规则**

比如： 

1. 从会员的角度来分析，商户订单交易类系统中查询会员某天某月某个订单，那么就需要按照会员结合日期来拆分， 不同的数据按照会员 ID 做分组，这样所有的数据查询 join 都会在单库内解决；
2. 如果从商户的角度来讲，要查询某 个商家某天所有的订单数，就需要按照商户 ID 做拆分；
3. 但是如果系统既想按会员拆分，又想按商家数据，则会有 一定的困难。如何找到合适的分片规则需要综合考虑衡量。 

 几种典型的分片规则包括： 

• 按照用户 ID 求模，将数据分散到不同的数据库，具有相同数据用户的数据都被分散到一个库中； 

• 按照日期，将不同月甚至日的数据分散到不同的库中； 

• 按照某个特定的字段求摸，或者根据特定范围段分散到不同的库中。

---

**优点：**
• 拆分规则抽象好，join 操作基本可以数据库做；
• 不存在单库大数据，高并发的性能瓶颈；
• 应用端改造较少；
• 提高了系统的稳定性跟负载能力。
**缺点：**
• 拆分规则难以抽象；
• 分片事务一致性难以解决；
• 数据多次扩展难度跟维护量极大；
• 跨库 join 性能较差。

### 6.1.3数据切分的原则

第一原则：能不切分尽量不要切分。
第二原则：如果要切分一定要选择合适的切分规则，提前规划好。
第三原则：数据切分尽量通过数据冗余或表分组（Table Group）来降低跨库 Join 的可能。
第四原则：由于数据库中间件对数据 Join 实现的优劣难以把握，而且实现高性能难度极大，业务读取尽量
少使用多表 Join。























