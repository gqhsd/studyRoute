# Redis

## 注意点:

### **避免大key**

原因:

1.  因为在集群环境下，如果某个 key 太大->数据导致迁移卡顿。
2. 在内存分配上，一个 key 对应结构太大，当它需要扩容时，会一次性申请更大的一块内存->导致卡顿。
3. 大 key 被删除，内存会一次性回收，再一次卡顿

**现象**:Redis 的内存大起大落

**定位大key**:

 scan 指令，扫出来的每一个key，使用 type 指令获得 key 的类型，然后使用相应数据结构的 size 或者 len 方法来得到大小，对于每一种类型，保留大小的前 N 名作为扫描结果展示出来

redis-cli -h 127.0.0.1 -p 7001 –-bigkeys (担心大幅抬升 Redis 的 ops 导致线上报警，可以增加一个休眠参数,如下)

redis-cli -h 127.0.0.1 -p 7001 –-bigkeys -i 0.1  每扫描100,休眠0.1秒

有时间阅读一下文章,目前还没看:https://mp.weixin.qq.com/s/ufoLJiXE0wU4Bc7ZbE9cDQ

### 单线程

单线程模型->小心指令时间复杂度为 O(n) ,可能导致redis卡顿

但其实还有异步线程

**主线程**:处理网络收发和执行命令

**异步线程**:处理耗时工作,如**AOF Sync**,后台删除大key等等

## 扩容过程

Redis就是数组+链表,扩容过程:重新分配一个2 倍大小的数组，所有的元素全部 rehash(hash值对数组长度取模)。因为数组的长度是 2^n 次方，所以取模运算等价于**hash值与数组长度进行位与**操作(为啥呢).

因为长度变化✖️2，数组长度的二进制最高位多了一个1,hash值位与的时候最高位多一个0,可能是1,然后按照高位进位加法,正好分布相邻

<img src="/Users/wzw/Desktop/assets/image-20230108225509444.png" alt="image-20230108225509444"  />

按照这样遍历,无论是缩容还是扩容都不会遗漏

## 数据结构

### 特点:

1. 数据容器不存在则创建,元素没有了则删除
2. 过期时间使整个hash结构的过期时间,而不是某个子元素

### String

存储到取出就是一次**序列化**和**反序列化**的过程

**底层数据结构**:类似ArrayList(字符数组)

特点:**预分配冗余空间**(但其实一般不会预分配冗余空间,因为不会append),**扩容机制->小于1M,每次双倍,大于1M,每次增加1M**

### List

类似Java的**LinkedList**,(双向链表),意味着**增删快,查找慢**,因为双向,因为**头尾最快,适合存储热点新闻之类的**

衍生出的数据结构: **栈 队列**

**ltrim**:保留中间的一段链表

**底层数据结构**:

快速链表:quickList,一块块连续的内存,中间用指针双向连起来

**好处**:快速插入删除,并且不出现太多用于存放指针的空间冗余



### Hash

数据结构:和HashMap一致,数组加链表

特点:不同于HashMap的一次性rehash,redis是渐进rehash:保留新旧hash结构,查询时同时查询两个hash结构,后续把旧的hash结构的内容迁移到新的,然后回收

**优点:**不用一次性序列化,反序列化全部数据,节省流量

缺点:存储消耗高于字符串(这里的消耗不知道具体指什么消耗)



### Set

数据结构:相当于Java的HashSet

衍生出的数据结构:zset

#### zset(跳跃列表)

SortSet和HashMap的结合体

特点:

- 是Set,保证了value唯一性

- 每个value可以赋予权重(score)用于排序

用途示例: value:用于id  ,score:关注时间

**底层数据结构:跳跃列表**

原因:为快速查找,一般使用二分查找,但链表不支持,因此引入跳跃链表,**类比中国地图,**找到浙江省,在找到宁波市,找到宁海县,因此**有的元素可以身兼数职,**在不同的层级,如宁波市

根据什么策略判定哪些元素在更高层呢?->**随机策略**,看运气

## 应用

### 分布式锁

流程:

用户要先set(表示我抢到了分布式锁),再expire(自动释放)

**问题**:两个流程非原子,中间被中断了就死锁了

**解决方案**:redis作者将expire设置为原子性操作

#### 超时问题

加锁和释放锁之间逻辑太长，以至锁过期了自动释放，第二个线程持有这把锁，

紧接着**第一个线程执行完了,把锁释放**，第三个线程就会拿到锁。

**解决方案:**

1. Redis 分布式锁**不要用于较长时间的任务**。如果出现,人工介入
2. **加锁时设置随机数,释放锁时随机数匹配再释放**,但此时匹配和释放非原子->**Lua脚本来实现原子**

**但这里说得好像有问题,第二种解决方案依然会导致,同一时间两个任务并行执行,如果两个任务是对同意块数据修改,会出现更新丢失的问题**

更好的解决方案在后面拓展里面有提到

#### 可重入性

类似Java 语言里的 ReentrantLock 

对客户端的 set 方法进行包装，使用线程的 Threadlocal 变量存储当前持有锁的计数 具体看书

#### 加锁没成功?

1. 直接抛出异常，通知用户稍后重试； 

2. sleep 一会再重试； 

3. 将请求转移至延时队列，过一会再试；

### 简单消息队列

特点:**简单,适用于一组消费组的情况,但是没有高级特性如ack,因此可靠性不高**

数据结构:List的队列类型

**流程:**rpush 和 lpop 结合使用,还可以使用 lpush 和 rpop 结合使用

**具体实现**:zset 的 value(任务)，到期处理时间作为 score，然后用**多个线程**轮询 zset 获取到期的任务进行处理，多个线程是为了**保障可用性**，挂了一个线程还有其它线程,但是要考虑并发争抢任务，确保任务不能被多次执行

多个线程在获取到任务后,要根据zrem再决定任务归属,这才是关键        

**问题**:队列空了怎么办,会陷入 pop 的死循环,拉高了客户端的 CPU，redis 的 QPS 也会被拉高

**解决方案:**

1. sleep一秒  缺点:延迟增大

2. blpop/brpop  队列没有数据的时候，会立即进入休眠状态，一旦数据到来，则立刻醒过来。消

   息的延迟几乎为零

#### **问题:空闲连接自动断开**

**问题**:线程一直阻塞，Redis 的客户端连接就成了闲置连接，闲置过久，服务器会主动断开连接，这个时候 blpop/brpop 会抛出异常来

**解决方案**:注意捕获异常，还要重试。



### 不精确的统计

如果是统计流量,一个计数器即可,但是如果要统计有多少用户进来呢?计数器?无法去重?每个页面一个set放用户id?浪费资源

HyperLogLog 提供**不精确**的去重计数方案->误差大概0.2

用法:

pfadd :类似集合,来一个用户id就往里放

pfcount:获取计数

pfmerge:合并



### 布隆过滤器(不精确的去重 刷抖音)

推荐给某个用户他没看过的视频怎么做? 能想到的传统场景不仅慢,耗性能,而且占用空间巨大

可以理解为**不精确的set结构,有contains方法,但是不是十分精确**

**特点**:布隆过滤器对于已经见过的元素肯定不会误判，它**只会误判那些没见过的元素**

优点:布隆过滤器存储元素的指纹(2字节),而不像set结构,需要元素的值,也就是字符串(可能几十上百字节),还需要在set引用的指针(4/8个字节)

再redis中作为插件

bf.add 添加元素

bf.exists  查询元素是否存在

bf.madd 批量添加元素

bf.mexists 一次查询多个元素是否存在

**拓展:** 

- 默认的布隆过滤器 误判在1%多点,可以**自定义参数**

- error_rate 不会因为数量超出就急剧增加，这就给我们重建过滤器提供了较为宽松的时间

使用方式:

1. 直接bf.add 创建默认布隆过滤器
2. add 之前使用 bf.reserve指令显式创建 参数如下
   1. key  就是key值
   2. error_rate **错误率**越小 所需空间越大 **默认0.01**
   3. initial_size **预计放入的元素数量**，当实际数量超出这个数值时，误判率会上升 **默认100**

**底层公式**:

- 误判率公式  输入两个参数:**预计元素的数量 n，错误率 f**,得到两个输出,**位数组的长度 l**(存储空间大小 (bit))，**hash 函数的最佳数量 k**
- **实际元素超出时,误判率变化曲线**,需要看书,误判率升的还是挺快的

**注意事项**

 initial_size 估计的过大，会浪费存储空间，估计的过小，就会影响准确率，在使用之前估计好元素数量，并加上一定的冗余空间。

布隆过滤器的 error_rate 越小，需要的存储空间就越大，对于不需要过于精确的场合，error_rate 设置稍大一点也无伤大雅,比如刷抖音

**底层原理**

add: 一个大型的位数组和几个不一样的无偏 hash 函数(元素的 hash 值算得比较均匀)。add  key 时，多个 hash 函数对 key 进行 hash 算得一个整数索引值然后对位数组长度进行取模得到一个位置,置1，每个 hash 函数hash得到的位置不一样。

contains:也是多个hash 函数对进行 hash 算得一个整数索引值然后对位数组长度进行取模得到位置,只要有一个不为1,就表示不包含(**所以只会误判那些没见过的元素**)

**实际应用**:

- 爬虫,爬过的URL不用重复去爬,去set去存几千万数据很离谱
- HBase,可以显著降低数据库的 IO 请求数量,先过滤掉大量不存在的row 请求，然后再去磁盘进行查询。

### 漏斗限流

Redis-cell:提供了原子的限流指令

 cl.throttle laoqian:reply 15 30 60 ->60s 最多 30 次(漏水速率)，漏斗的初始容量为 15

返回值是数组,5个整数

\1) (integer) 0 # 0 表示允许，1 表示拒绝

\2) (integer) 15 # 漏斗容量 capacity

\3) (integer) 14 # 漏斗剩余空间 left_quota

\4) (integer) -1 # 如果拒绝了，需要多长时间后再试(漏斗有空间了，单位秒)  -1表示没有被拒绝

\5) (integer) 2 # 多长时间后，漏斗完全空出来(left_quota==capacity，单位秒)



### 地理位置-GeoHash

假设数据库存储一系列经纬度,和一个中心点,然后计算这个坐标附近的其它元素，按照距离进行排序,怎么做?

1. 遍历来计算所有的元素和目标元素的距离然后再进行排序->计算量太大
2. 矩形区域来限定元素的数量,然后步骤1,若结果不满意,重新划定矩形区域,然后步骤1

但是高并发场景要频繁查询,可能数据库性能不够

**GeoHash**:

把地图分成颗粒度极小的网格,如四宫格,左下角00,右上角11,那么经纬度可以得到较长二进制数字(越长表示越精确),这个整数表示的不是到元带你的距离,而是网格上的位置,因为数字相近的也表示两者之间位置相近,redis中是52位长度,经过base32编码后放到zset中,编码值进行排序,因此score相近就是表示位置相近

redis可以将编码值还原得到原始经纬度

命令:

-  geoadd company 116.48105 39.996794 juejin  ->geoadd:添加 company:集合名  juejin:地点名
- geodist company juejin ireader km->geodist 获取距离  km:长度单位,其他还有m,ml(英里),ft(尺)
- geopos company juejin ireader->geopos 获取原始经纬度  可以获取多个
-  geohash company ireader->获取编码值 可以去http://geohash.org/${hash} 得到具体定位
- georadiusbymember company ireader 20 km count 3 asc->范围 20 公里以内最多 3 个元素按距离正排，它不会排除自身
- georadiusbymember company ireader 20 km withcoord withdist withhash count 3 asc->三个可选参数 withcoord:显示经纬度 withdist:显示距离 withhash:显示二进制数字的十进制数字
-  georadius company 116.514202 39.905409 20 km withdist count 3 asc->根据坐标值来查询附近的元素,参数和 georadiusbymember 基本一致

**注意事项:**

1. 实际使用可能数据量很大,集群环境中集群可能会迁移,数据过大可能卡顿,建议一个zset数据量不好过1m
2. Geo 的数据使用单独的 Redis 实例部署，不使用集群
3. 数据量过亿甚至更大,按省市区拆分,降低单个 zset 集合的大小



### Scan

普通使用正则查询:

keys * ;  //得到符合条件的key

keys codehole*; 

keys code*hole

缺点:

- 没有 offset、limit 参数,可能得到上百万个key
- keys是遍历, 复杂度O(n)，效率很低

**Scan:**

- 复杂度O(n),但是通过游标分步进行,不会阻塞
- 提供 limit 参数
- 可以正则
- **结果可能重复,需要去重**
- 返回结果为空不一定结束遍历,得看返回的游标是不是0

**使用:**

三个参数 

1. cursor 整数值 //游标值,下一次查询带上,直到0结束
2. key 的正则模式
3. 遍历的 limit hint //查询范围,返回的数可能小于该数字

```
示例:

scan 0 match key99* count 1000 //从0开始 模糊查询 1000个数据里找到符合条件的

返回:

1. 游标值
2. key列表
   1. ...
   2. ...
```

**底层原理:**

**redis其实就是个链表+数组**,,数组上连接的链表的值就是具体结构的指针,scan返回的值就是这个数组在链表中的下标,但是注意,scan不是从下标0到结尾,而是采用**高位进位加法**:用二进制数组来看普通遍历是从右往左,高位进位加法则是(000->100->010->110->001,为什么是这么遍历跟扩容缩容有关系,可以看最顶上的缩容),**原因**:扩容/缩容时数组长度✖️2/➗2,那么右边就会多/少一位,由左往右遍历就不会重复遍历/遗漏

**更多指令:**

scan->遍历所有的 key

zscan ->遍历 zset 集合元素

hscan ->遍历 hash 字典的元素

sscan ->遍历 set 集合的元素。

## 原理

### **线程** **IO** 模型

**要点:**

1. redis,nginx都是**单线程**模型->小心指令时间复杂度为 O(n) ,可能导致redis卡顿

问题:

redis是**单线程为什么这么快**?->所有的数据都在内存,所有的运算都是内存级别的运算

单线程如何处理**那么多的并发客户端连接**->涉及到多路复用,select系列的事件轮询 API,非阻塞 IO

#### 非阻塞 IO

阻塞 IO: socket的read和write方法默认是阻塞的:read方法传入参数n(要读取n个字节),读不满就阻塞,除非后续有数据到来读满或连接断开,write 方法一般来说不会阻塞，除非内核为socket分配的写缓冲区满了

非阻塞 IO :socket对象上提供了一个选项 Non_Blocking，打开时，读写不会阻塞，能读多少读多少，能写多少写多少。能读多少取决于内核为socket分配的读缓冲区数据字节数，能写多少取决于内核为socket分配的写缓冲区的空闲空间字节数。读方法和写方法通过返回值来告知程序实际读写了多少字节。

#### 事件轮询(多路复用)->NIO 

**问题:** 

非阻塞 IO 要读数据，结果读了一部分就返回了，什么时候继续读->数据到来时，线程如何得到通知。

写也是一样，如果缓冲区满了，写不完，什么时候继续写->线程也应该得到通知。

**解决方案:**    事件轮询 API

select():入参:读写描述符列表 出参:可读可写事件

线程通过调用一个函数获取事件进行处理,函数有一个 timeout (阻塞等待事件到来)，一旦期间有任何事件到来，函数立即返回该事件有给线程处理。若没有任何事件到来，也会立即返回。处理完了继续过来轮询。于是线程就进入了一个死循环，一个循环为一个周期

**select已经弃用**,epoll(linux)和 kqueue(freebsd & macosx)->弃用原因:描述符特别多(也就是连接数很多,也就是多线程情况)时性能很差

#### 指令队列

每个客户端套接字连接关联一个指令队列。指令通过队列来排队进行顺序处理

#### 响应队列

每个客户端套接字连接关联一个响应队列,来将指令结果回复给客户端

如果响应队列为空，表示目前没有写事件要获取，可以将当前的客户端描述符从 write_fds 里面移出来(也就是说当前线程退出select系统的轮询)。等有数据了，再将描述符放进去。避免 select 系统调用立即返回写事件，结果发现没什么数据可以写(也就是空轮询太多次)。CPU会升高

#### 定时任务

redis是单线程的,但是redis除了处理客户端的指令,本身还有定时任务需要去处理,怎么解决呢?

定时任务会记录**最小堆**中。这个堆中，最快要执行排在堆的最上方。在每个循环周期，Redis 都会将最小堆里面已经到点的任务立即处

理。处理完毕后，将最快要执行的任务还需要的时间记录下来，这个时间就是 select 系统调用的 timeout 参数。因为 Redis 知道未来 timeout 时间内，没有其它定时任务需要处理，可以安心睡眠 

### 通信协议

采用文本协议

格式:

- 单行字符串 以 + 符号开头。 -> +hello world\r\n
- 多行字符串 以 $ 符号开头，后跟字符串长度。 ->$11\r\nhello world\r\n
- 整数值 以 : 符号开头，后跟整数的字符串形式。-> :1024\r\n
- 错误消息 以 - 符号开头。->   -WRONGTYPE Operation against a key holding the wrong kind of value
- 数组 以 * 号开头，后跟数组的长度。-> *3\r\n:1\r\n:2\r\n:3\r\n
- 其他类型:
  - NULL: 多行字符串表示,只是长度-1 -> $-1\r\n
  - 空串:多行字符串表示,只是长度0 -> $0\n\r\n  为什么是两个\r\n? 因为两个\r\n 之间,隔的是空串。

客户端-> 服务器 :只有多行字符串数组

示例: set author codehole-> *3\r\n$3\r\nset\r\n$6\r\nauthor\r\n$8\r\ncodehole\r\n

服务器-> 客户端: 多种格式

### 持久化

Redis 的持久化机制有两种: 快照(全量备份)和AOF 日志(连续的增量备份)

快照是内存数据的二进制序列化形式，在存储上非常紧凑

AOF 日志记录的是内存数据修改的指令记录,在长期的运行过程中会变的无比庞大，数据库重启时需要加载 AOF 日志进行指令重放，时间会无比漫长->需要定期进行 AOF 重写，给 AOF 日志进行瘦身。

#### **快照原理**

问题:

1. 单线程,却要在处理客户端请求的同时,进行持久化到硬盘,但是文件IO操作是不能和请求一起用多路复用API进行处理的,也就是一边处理请求,一边IO
2. 持久化的同时，内存数据结构还在改变,如一个大型的 hash 字典正在持久化，结果一个请求把它给删掉

解决方案:

**操作系统的多进程 COW(Copy On Write) 机制**

**fork(多进程)**:

持久化时会调用 glibc 的函数 fork 产生一个子进程,子进程刚刚产生时，它和父进程**共享内存**里面的代码段和数据段(**为了节约内存资源**)。**子进程做数据持久化，不会修改内存数据结构，它只进行遍历读取**，序列化到磁盘中.

接下来就会用到**COW机制**.数据段是由操作系统的很多的页面组成，父进程对其中一个页面的数据进行修改时，会将从**共享内存的代码段和数据段复制**一份出来，然后进行修改,子进程就读原来共享内存的代码段和数据段,数据没变化。**子进程把数据保存在硬盘的临时文件中,读取完成后才会把这个临时文件替换以前的快照.因此任何时候,快照文件都是完整的**

解释:

父进程进行fork后,会出现一个子进程,也会执行剩下的代码,此时相当于链表,父进程的fpid指向子进程地址,子进程没有子进程,fpid=0,fpid<0表示fork失败

```
fpid = fork();
    if (fpid < 0)
        printf("error in fork!");
    else if (fpid == 0)
    {
        printf("i am the child process, my process id is %d/n", getpid());
        printf("我是爹的儿子/n");//对某些人来说中文看着更直白。
        count++;
    }
    else
    {
        printf("i am the parent process, my process id is %d/n", getpid());
        printf("我是孩子他爹/n");
        count++;
    }
```

#### **AOF 原理**

存储的是 Redis 服务器的指令序列，只记录对内存进行**修改**的指令记录.

假设 AOF 日志记录了自 Redis 实例创建以来所有的修改性指令，那么对一个空的 Redis 实例顺序执行所有的指令(重放)，来恢复 Redis 当前实例的内存数据

Redis 收到指令后，先参数校验，如果没问题，就存储到 AOF 日志中，也就是**先存到磁盘，然后再执行指令**->即使遇到突发宕机，可以通过指令重放恢复到宕机前的状态

**问题:**

长期运行后，AOF 的日志会越变越长,重放过程很耗时，导致长时间 Redis 无法对外提供服务.

**解决方案:**

对AOF日志进行瘦身-> AOF 重写



**AOF 重写**

1. 开辟一个子进程对内存数据遍历转换成一系列 Redis 的操作指令，序列化到一个新的 AOF 日志文件中。

2. 完毕后再将操作期间发生的增量 AOF 日志追加到这个新的 AOF 日志文件中，追加完毕后替代旧的 AOF 日志就完成了

**问题**

AOF 日志是文件，当程序对文件进行写操作时，实际上是将内容写到了内核为文件描述符分配的一个**内存缓存中**，然后内核会**异步将脏数据**刷回到磁盘的,如果中间宕机,缓存中的数据没有完全刷到硬盘,即**日志丢失**,怎么办?->**fsync**

**fsync**

Linux 的 glibc 提供了 fsync(int fd)函数可以将指定文件的内容强制从内核缓存刷到磁盘。只要 Redis 进程实时调用 fsync 函数就可以保证 aof 日志不丢失。

**补充:**fsync是耗时操作,为不影响主线程,redis把他封装成任务丢到后台线程异步执行

**问题:**

 fsync 是一个磁盘 IO 操作，**它很慢**！如果 Redis 执行一条指令就要 fsync 一次，那么效率非常低

**解决方案**

是每隔 1s 左右(可以配置)执行一次 fsync 操作,既保证高性能,也保证数据少丢失(注意不是完全不丢失)

另外的方案(基本不用):

- 永不 fsync(不安全)
- 来一个指令就 fsync 一次(非常慢)

#### 运维

持久化是通过开启子进程进行的->比较耗资源

 1、快照->遍历整个内存，大块写磁盘会加重系统负载

 2、AOF 的 fsync 是一个耗时的 IO 操作，它会降低 Redis 性能，增加系统 IO 负担

因此持久化工作主要放在从节点进行

但是如果出现网络分区，从节点长期连不上主节点，就会出现数据不一致的问题，特别是网络分区情况下又不小心主节点宕机了，那么数据就会丢失，所以在生产环境要做好**实时监控**工作另外还应该再**增加一个从节点**以降低网络分区的概率，只要有一个从节点数据同步正常，数据也就不会轻易丢失

#### **Redis 4.0混合持久化**

问题:

- 重启 Redis 时，若使用快照恢复内存状态->会丢失大量数据。

- 通常使用 AOF 日志重放，但是重放性能相对慢很多,要花费很长的时间

解决方案:

将 **rdb 文件的内容和增量的 AOF 日志**存在一起。这里的 AOF 日志不是全量的，而是自持久化开始到持久化结束中间发生的增量 AOF 日志.重启的时候，可以先加载 rdb 的内容，然后再重放增量 AOF 日志就可,效率很高



### 管道

客户端与服务器交互,是一次次的网络数据包在管道内的来回,一个指令一个写-读的过程,但是如果是两条指令,即写-读-写-读,4个网络来回.

但是**如果换一下顺序**,写-写-读-读,**两次写和读放在一个来回里**,可以大幅节省IO时间,管道中指令越多，效果越好

#### **深入理解管道本**质

一个指令,即一个写-读流程的完整步骤

下面步骤反映出网络IO大致步骤,数据从网卡放到缓冲区->从缓冲区放到进程

1、客户端进程调用 write 将消息写到操作系统内核为套接字分配的发送缓冲区 send buffer。

 2、客户端操作系统内核将发送缓冲的内容发送到网卡，网卡硬件将数据通过「网际路由」送到服务器的网卡。

 3、服务器操作系统内核将网卡的数据放到内核为套接字分配的接收缓冲 recv buffer。

 4、服务器进程调用 read 从接收缓冲中取出消息进行处理。

 5、服务器进程调用 write 将响应消息写到内核为套接字分配的发送缓冲 send buffer。

 6、服务器操作系统内核将发送缓冲的内容发送到网卡，网卡硬件将数据通过「网际路由」送到客户端的网卡。

 7、客户端操作系统内核将网卡的数据放到内核为套接字分配的接收缓冲 recv buffer。

 8、客户端进程调用 read 从接收缓冲中取出消息返回给上层业务逻辑进行处理。



 write 操作不是等到对方收到消息才会返回，但实际上,write操作只负责将数据写到本地操作系统内核的发送缓冲然后就返回了。剩下的事交给操作系统内核异步将数据送到目标机器。但是如果发送缓冲满了，那么就需要等待缓冲空出空闲空间来，这个就是写操作 IO 操作的真正耗时。

 read 操作不是从目标机器拉取数据，但实际上,read 操作只负责将数据从本地操作系统内核的接收缓冲中取出来即可。但是如果缓冲是空的，那么就需要等待数据到来，这个就是读操作 IO 操作的真正耗时。

所以用简单的get指令来说，write 操作几乎没有耗时，直接写到发送缓冲就返回，而 read 比较耗时，因为它要等待消息经过网络路由到目标机器处理后的响应消息,再回送到当前的内核读缓冲才可以返回。这才是一个网络来回的真正开销。

而对于管道来说，连续的 write 操作根本就没有耗时，之后第一个 read 操作会等待一个网络的来回开销，然后所有的响应消息就都已经回送到内核的读缓冲了，后续的 read 操作直接就可以从缓冲拿到结果，瞬间就返回了。

### **事务**

事务模型很简单,因此不严格

multi->exec 开始事务->执行 所有的指令在 exec 之前缓存在服务器的一个事务队列中，服务器收到 exec 指令，才执行事务队列，结束后一次性返回运行结果。因为 单线程特性，执行队列过程中不会被其它指令打搅

multi->discard 开始事务->丢弃

**注意:**

redis的事务碰到错误是**不会回滚**的,只会保证中间的指令不会被其他指令打断

#### watch

客户端需要并发对redis存储的数据进行修改时怎么避免冲突?

- 分布式锁,但这是悲观锁
- watch,乐观锁

watch大致流程

```
while True:
 do_watch() //事务开始之前盯住 1 个或多个关键变量
 commands()
 multi()
 send_commands()
 try:
 	exec()//事务执行时，服务器(注意是服务器,也就是redis做的)获取到要执行事务队列时，Redis 会检查关键变量自 watch 之后,是否被
					修改了 (包括当前事务所在的客户端)。如果关键变量被动过，exec 方法就会返回 null 回复告知事务执行失败，这个时候客户端一般					会选择重试
 	break
 except WatchError:
 	continue
 	
注意:Redis 禁止在 multi 和 exec 之间执行 watch 指令，而必须在 multi 之前做好盯住关键变量，否则会出错
```

示例代码

```
public static int doubleAccount(Jedis jedis, String userId) {
 String key = keyFor(userId);//得到key
 while (true) {
 	jedis.watch(key);
 	int value = Integer.parseInt(jedis.get(key));
 	value *= 2; // 加倍
 	Transaction tx = jedis.multi();
 	tx.set(key, String.valueOf(value));
 	List<Object> res = tx.exec();
 	if (res != null) {
 		break; // 成功了
 	}
 }
 return Integer.parseInt(jedis.get(key)); // 重新获取余额
 }
```

### PubSub-支持多播的消息队列(基本废弃)

什么是多播:允许生产者生产一次消息，中间件负责将消息复制到多个消息队列，每个消息队列由相应的消费组消费。支持了消息多播，多个消费组的逻辑就可以放到不同的子系统中

为支持多播,redis提供了额外的模块PubSub来支持

示例代码如下:

```
消费者
# -*- coding: utf-8 -*-
import time
import redis
client = redis.StrictRedis()
p = client.pubsub()
p.subscribe("codehole") //表示订阅codehole
while True:
 msg = p.get_message()//获取订阅的主题是否有消息,若没有则休眠一秒钟,可以使用 listen 来阻塞监听消息,这和消息队列的blpop 来代替													休眠很像
 if not msg:
 time.sleep(1)
 continue
 print msg
 
阻塞消费者
# -*- coding: utf-8 -*-
import time
import redis
client = redis.StrictRedis()
p = client.pubsub()
p.subscribe("codehole")
for msg in p.listen():
 print msg
 
生产者
# -*- coding: utf-8 -*-
import redis
client = redis.StrictRedis()
client.publish("codehole", "python comes")
client.publish("codehole", "java comes")
client.publish("codehole", "golang comes")
```

**主题:**

上面的是基于名称来订阅消息发布消息,同一个名称下可以有不同的主题

```
 subscribe codehole.image codehole.text codehole.blog //codehole是名称,image是主题
```

问题:如果要**增加一个主题** codehole.group，客户端必须调用新的订阅指令才可以收到新主题的消息推送

解决方案: redis 提供了**模式订阅**功能 ->psubscribe codehole.* 

#### 消息结构

```
{'pattern': None, 'type': 'message', 'channel': 'codehole', 'data': 'python comes'}
data :消息的内容
channel :主题名称。
type :消息类型，普通消息->message;控制消息，比如订阅指令的反馈-> subscribe;模式订阅的反馈->psubscribe;取消订阅指令的反馈 ->unsubscribe 和 punsubscribe。
pattern :当前消息是使用哪种模式订阅到的，如果是通过 subscribe 指令订阅的，这个字段就是空。
```

**缺点:**

生产者传递一个消息，Redis 会直接找到相应的消费者传递过去。若没有消费者,消息丢弃,一个消费者断连后**重新连接**,**中间的消息丢失**

如果 Redis 停机重启，PubSub 的消息是不会持久化的,消息直接被丢弃。

### 小对象压缩

**小对象压缩存储(ziplist)**

如果集合数据结构很小，会使用紧凑存储形式压缩存储,参考HashMap 本来是数组+链表/红黑树，但是如果内部元素比较少，不如使用一维数组进行存储，不仅节约空间,且遍历很快

**ziplist** 

一个紧凑的**字节数组结构**，如下图所示，每个元素之间都是紧挨着的。

<img src="../../../Desktop/assets/image-20230122002134531.png" alt="image-20230122002134531" style="zoom:50%;" />

- 存储的是 hash 结构->key 和 value 会作为两个 entry 相邻存在一起。
- 存储的是 zset，那么 value 和 score 会作为两个 entry 相邻存在一起。

**intset**

一个紧凑的整数数组结构，它用于存放元素都是整数的并且元素个数较少的 set 集合

如果整数可以用 uint16 表示，那么 intset 的元素就是 16 位的数组，如果新加入的整数超过了 uint16 的表示范围，就使用 uint32 表示,Redis 支持 set 集合动态从 uint16 升级到 uint32，再升级到 uint64

**hashtable**

如果 set 里存储的是字符串，那么 sadd 立即升级为 hashtable 结构。还记得 Java 的HashSet 么，它内部是使用 HashMap 实现的。

**综上所述,redis会根据存储的数据结构类型和数据类型,来变换小对象压缩存储的结构**



**存储界限** 当集合对象的元素不断增加，或者某个 value 值过大，小对象存储也会被升级为标准结构。

Redis 规定在小对象存储结构的限制条件如下：

```
hash-max-zipmap-entries 512 # hash 的元素个数超过 512 就必须用标准结构存储
hash-max-zipmap-value 64 # hash 的任意元素的 key/value 的长度超过 64 就必须用标准结构存储
list-max-ziplist-entries 512 # list 的元素个数超过 512 就必须用标准结构存储
list-max-ziplist-value 64 # list 的任意元素的长度超过 64 就必须用标准结构存储
zset-max-ziplist-entries 128 # zset 的元素个数超过 128 就必须用标准结构存储
zset-max-ziplist-value 64 # zset 的任意元素的长度超过 64 就必须用标准结构存储
set-max-intset-entries 512 # set 的整数元素个数超过 512 就必须用标准结构存储
```

### 内存回收机制和内存分配算法

不总是可以将空闲内存立即归还给操作系统,为什么?

操作系统回收内存是**以页为单位**，如果这个页上只要有一个 key 还在使用，就不能被回收。若Redis 删除了 1GB 的 key，这些 key 分散到了很多页面中，每个页面都还有其它 key 在使用，内存就不会立即被回收

**内存分配算法**

内存分配很复杂，需要适当的算法划分内存页，需要考虑内存碎片，需要平衡性能和效率

redis使用第三方内存分配库jemalloc(可以切换到 tcmalloc)

后续需要去看下,**面试可能会问**

### 主从同步

有了主从，当 master 挂掉的时候，从库过来接管，服务就可以继续，否则 master 需要经过数据恢复和重启的过程，这就可能影响线上业务。

#### CAP原理

-  **C** - **C**onsistent ，一致性 :数据一致性

-  **A** - **A**vailability ，可用性:系统可用性

-  **P** - **P**artition tolerance ，分区容忍性

  一句话概括 CAP 原理就是——**网络分区发生时，一致性和可用性两难全**:若保证一致性,必须暂停服务同步数据;若保证可用性,提供的数据可能不一致

**最终一致**

1. Redis 的主从数据是**异步同步**->不是强一致性
2. 保证**最终一致性**,网络恢复后,从节点会努力追赶主节点，最终从节点的状态会和主节点的状态将保持一致

#### 增量同步

Redis **同步的是指令流**，主节点会将修改型指令在本地的内存 buffer 中，然后异步将 buffer 中的指令同步到从节点，从节点一边执行同步的指令流，一遍向主节点反馈自己同步到哪里了 (偏移量)。

因为内存的 buffer 是有限的，所以 Redis 主库不能将所有的指令都记录在内存 buffer 中。 buffer 是一个**定长的环形数组**，如果数组内容满了，就会从头开始覆盖前面的内容。因此如果网络断开导致从节点没有及时同步,**有些指令可能会被覆盖**,就要用到更复杂的同步机制-**快照同步**

#### 快照同步

1. 主库上进行一次 bgsave 将当前内存的数据全部快照到磁盘文件中
2. 将快照文件的内容全部传送到从节点
3. 从节点接受完毕后，将当前内存的数据清空,立即执行一次全量加载，加载完毕后通知主节点继续进行增量同步。

**注意事项(问题)**:

快照同步进行的过程中，主节点的复制 buffer 还在不停移动，如果快照同步的时间过长或者复制 buffer 太小，都会导致同步期间的增量指令在复制 buffer 中被覆盖，这样就会导致快照同步完成后无法进行增量复制，然后会再次发起快照同步，如此极有可能会陷入快照同步的死循环。

**解决方案:**

配置一个合适的复制 buffer 大小参数,避免死循环

**增加从节点**

当从节点刚刚加入到集群时，先要进行一次快照同步，再继续进行增量同步。

#### 无盘复制

快照同步时，文件 IO 操作消耗很大，特别是当系统正在进行 AOF 的 fsync 操作时如果

发生快照，fsync 将会被推迟执行，这就会严重影响主节点的服务效率。

 Redis 后续开始支持无盘复制:主服务器直接通过套接字将快照内容发送到从节点，生成快照是一个遍历的过程，主节点会一边遍历内存，一遍将序列化的内容发送到从节点，从节点还是跟之前一样，先将接收到的内容存储到磁盘文件中，再进行一次性加载。

#### Wait指令

wait 指令可以让异步复制变身同步复制，确保系统的强一致性 (不严格)

wait 提供两个参数，第一个参数是从库的数量 N，第二个参数是时间 t，以毫秒为单位。它表示等待 wait 指令之前的所有写操作同步到 N 个从库 (也就是确保 N 个从库的同步没有滞后)，最多等待时间 t。如果时间 t=0，表示无限等待直到 N 个从库同步完成。

假设此时出现了网络分区，wait 指令第二个参数时间 t=0，主从同步无法继续进行，wait 指令会永远阻塞，Redis 丧失可用性

## 集群

### Sentinel(哨兵)

**问题:**

主从同步只是保证数据的一致性,但是没有做到高可用,万一主库宕机,依靠人工手动切库不现实

**解决方案:**

redis提供了哨兵,可以自动主从切换

<img src="../../../Desktop/assets/image-20230124001904556.png" alt="image-20230124001904556" style="zoom:50%;" />

可以将 Redis Sentinel 集群看成是一个 ZooKeeper 集群，它一般是由 3～5 个节点组成，这样挂了个别节点集群还可以正常运转。

它负责持续**监控主从节点的健康**，当主节点挂掉时，自动选择一个最优的从节点切换为主节点。客户端来连接集群时，会首先连接 sentinel，通过 sentinel 来查询主节点的地址，然后再去连接主节点进行数据交互。当主节点发生故障时，客户端会重新向 sentinel 要地

址.当原来的主节点恢复后就会成为从节点去主节点那里进行同步

**消息丢失**

Redis 主从采用异步复制，意味着当主节点挂掉时，主从节点数据可能不一致。如果主从延迟特别大，丢失的数据就特别

多。**Sentinel 只能保证消息少丢失**。它有两个选项可以限制主从延迟过大。

注意:这个参数只是限制主从延迟过大,**如果不满足条件就直接丧失可用性**

```
min-slaves-to-write 1 //至少一个从节点在正常复制
min-slaves-max-lag 10  //10s没有收到反馈就认定这从节点没有正常复制
```

#### 基本使用

注意这个连接池是客户端维护的

```
>>> from redis.sentinel import Sentinel
>>> sentinel = Sentinel([('localhost', 26379)], socket_timeout=0.1)  //连接sentinal集群
>>> sentinel.discover_master('mymaster')//得到主节点地址和端口
('127.0.0.1', 6379)
>>> sentinel.discover_slaves('mymaster')//得到从节点地址和端口
[('127.0.0.1', 6380)]
```

```
>>> master = sentinel.master_for('mymaster', socket_timeout=0.1)//客户端从连接池获取主节点连接
>>> slave = sentinel.slave_for('mymaster', socket_timeout=0.1)//客户端从连接池获取从节点连接,从地址有多个，redis 客
户端对从地址采用轮询方案-轮着来
>>> master.set('foo', 'bar')
>>> slave.get('foo')
'bar
```

问题:

sentinel 进行主从切换时，客户端如何知道地址变更了 ? 

1. **客户端的**连接池建立新连接时,会去查主库地址,跟内存的比对,变更了，就断开所有连接，重新使用新地址建立新连接。
2. 若没有新链接建立,原来的主库还能用,只是sentinel主动主从切换了,之前的主库被降级到从库，所有的修改性的指令都会抛出 ReadonlyError,(谁?)处理命令的时候捕获了一个特殊的异常 ReadOnlyError，在这个异常里将所有的旧连接全部关闭了，后续指令就会进行重连,如果没有修改性指令，连接不会切换，但是数据不会有变化，所以不切换也没关系

### Codis(Redis 集群方案之一)

背景:

大数据高并发场景下,单个redis不够用,原因如下:

- Redis 的内存不宜过大,否则快照文件rdb会过大,全量同步时耗时太长,指令重放也很消耗时间
- 单个 Redis 实例只能利用单个核心，单个核心要完成海量数据的存取和管理工作压力会非常大

解决方案:

Redis 集群,如Codis	

**特性:**

- 它是一个**代理中间件**，它和 Redis 一样使用 Redis 协议对外提供服务，当客户端向 Codis 发送指令时，Codis 负责将指令转发到后面的 Redis 实例来执行，并将返回结果返回给客户端
- 客户端操纵codis和操纵redis的指令基本没区别
- 只是一个转发代理中间件，这意味着我们可以启动多个Codis 实例，供客户端使用，因为单个 Codis 代理能支撑的QPS 比较有限，多个 Codis 代理可以显著增加整体的 QPS 需求，还能起到容灾功能，挂掉一个 Codis 还有其他代理继续服务

#### **分片原理**:

Codis 要将特定的 key 转发到特定的 Redis 实例，这种对应关系 Codis 怎么管理?

1024 个槽位(默认,但是**可以配置**)，首先对key 进 行 crc32 运算计算哈希值，再将 hash 后的整数值对 1024 取模得到对应 key 的槽位

然后根据槽位和redis实例的对应关系找到实例,这个**对应关系维护在Codis的内存**中

**问题:**

上面说可以启动多个Codis 实例,不同的 Codis 实例之间槽位关系如何同步?

解决方案:

又需要一个**分布式配置存储数据库(Zookeeper和etcd )**专门用来持久化槽位关系

Codis 将槽位关系存储在 zk 中，并且提供了一个 watch 可以用来观察和修改槽位关系，当槽位关系变化时，Codis 会监听到变化并重新同步槽位关系

#### 扩容

增加redis节点**,槽位关系变化**,某些槽位上的**key要迁移到新的节点**上

**问题:**上面的算法只能根据key找到槽位,那么**怎么根据槽位获取到对应的key**呢?

**解决方案:**Codis 通过 SLOTSSCAN指令扫描出待迁移槽位的所有的 key，然后挨个迁移

**问题:**某个槽位上的key**正在从A前往B**,迁移过程中如果要获取指定key的数据,怎么知道在A还是在B?

**解决方案:****立即强制**对当前的单个 key 进行迁移，迁移完成后，再将**请求转发到新的 Redis 实例**

注意事项:

Scan 指令都是无法避免重复的(上面Scan章节有提到),但是不影响迁移,因为单个key迁移一次后,旧的实例就把他删掉了,也就扫不出来了

#### 自动均衡

有些redis实例对应的槽位多,有些少,自动均衡会在系统比较空闲时观察每个 Redis 实例对应的槽位 数量，如果不平衡，就会自动进行

迁移

#### Codis的缺点

1. key分散在不同实例上,因此不支持事务
2. 集群的迁移的最小单位是key,单个key对应的value不能过大,否则迁移卡顿(官方建议单个集合结构的总字节容量不要超过 1M)
3. Codis多了个中转层,网络开销大,性能略微降低
4. 需要维护zk

#### Codis的优点

1. 比 Redis Cluster 要简单很多

#### 批量获取多个key

这些key可能在不同实例上->将 key 按照所分配的实例打散分组，然后依次对每个实例调用 mget 方法，最后将结果汇总为一个

### Cluster

#### 特性:

- 去中心化,比如三个节点,每个节点存的数据不一样,它们之间通过一种特殊的二进制协议相互交互集群信息
- 划分为 16384 个 槽位,每个节点负责其中一部分槽位,**槽位的信息存储于每个节点中,不像 Codis需要另外的分布式存储来存储**
- 客户端来连接集群时，它也会得到一份集群的槽位配置信息->查找某个 key 时，可以直接定位到目标节点(不同于Codis 需要通过 Proxy 来定位目标节点)

-  Cluster 不支持事务

- Cluster 的 mget 方法相比 Redis 要慢很多，被拆分成了多个 get 指令

- Cluster 的 rename 方法不再是原子的，它需要将数据从原节点转移到目标节点

  问题:服务器的槽位信息配置可能变化->需要纠正机制来实现槽位信息的校验调整

#### **槽位定位算法**

- key 值使用 crc32 算法进行 hash 得到整数值,后对16384 取模得到槽位
- 强制某个key在特定槽位上

#### 跳转

当客户端向一个错误的节点发出了指令,节点发现指令的 key 所在的槽位并不归自己管理，这时会向客户端发送一个特殊的跳转指令携带目标操作的节点地址，让客户端跳转

```
GET x
-MOVED 3999 127.0.0.1:6381 //前面有减号->该get指令是一个错误消息,3999 是 x 对应的槽位编号,后面是目标节点地址
```

**客户端收到 MOVED 指令后，要立即纠正本地的槽位映射表。后续所有 key 将使用新的槽位映射表**

#### 迁移

槽位的分配情况会默认分配,redis-trib 可以手动调整槽位的分配情况

**迁移过程**

迁移单位是槽,当一个槽正在迁移时就处于**中间过渡**状态,在原节点的状态为 migrating，在目标节点的状态为 importing，表示数据正在从源流向目标

大致流程:**从源节点获取内容** **=>** **存到目标节点** **=>** **从源节点删除内容**

详细流程:redis-trib 在源和目标节点设置好中间过渡状态，然后一次性获取源节点槽位的所有 key 列表(keysinslot 指令，可以部分获取)，再挨个 key 进行迁移。每个 key 的迁移过程是以原节点作为目标节点的「客户端」，原节点对当前的 key 执行 dump 指令得到序列化内容，然后通过「客户端」向目标节点发送指令 restore 指令，目标节点反序列化将内容恢复到目标节点的内存中，然后返回「客户端」OK，原节点收到后再把当前节点的 key 删除掉

**注意点:**

**迁移过程是同步的**，在目标节点执行 restore 指令到原节点删除 key 之间，原节点的主线程会处于阻塞状态，直到 key 被成功删除

**迁移过程中出现网络故障:**两个节点依旧处于中间过渡状态,下次迁移工具重新连上时提示用户继续进行迁移

**迁移过程中，客户端访问的流程:**新旧两个节点对应的槽位都存在部分 key 数据,客户端先尝试访问旧节点,若没有,要么在新节点，要么根本就不存在,会向客户端返回一个**-ASK targetNodeAddr** 的重定向指令,客户端收到后向新节点执行一个不带任何参数的 asking 指令，然后在目标节点再重新执行原先的操作指令

**为什么需要执行一个不带参数的 asking 指令**:

迁移没有完成之前，按理说这个槽位还是不归新节点管理的,不发送的话会走上面的MOVED (跳转逻辑),形成重定向循环,asking 指令告诉节点下一条指令不能不理，而要当成自己的槽位来处理

**迁移是会影响服务效率**，同样的指令在正常情况下一个 ttl 就能完成，而在迁移中可能得 3 个 ttl 

**迁移感知**

上面的特性中提到,**客户端会保存槽位和节点的分配状态**,然后**直接找到**槽位所在的节点,那么这个**分配状态需要实时更新**

#### 容错

可以为每个主节点设置若干个从节点,主节点故障时，集群会自动将其中某个从节点提升为主节点。如果某个主节点没有从节点，它发生故障时，集群将处于不可用状态。

不过 Redis 提供了一个参数 cluster-require-full-coverage:允许部分节点故障，其它节点继续提供服务

#### 网络抖动

cluster-node-timeout:当某个节点持续 timeout 的时间失联时，才认定该节点出现故障，需要进行主从切换。

 cluster-slave-validity-factor :作为倍乘系数来放大这个超时时间。如果这个系数为零，主从切换是不会抗拒网络抖动的,大于 1，它就成了主从切换的松弛系数。

#### 可能下线与确定下线

一个节点认定某个节点失联只是可能下线,当大多数节点都认定了某个节点失联了，才认为该节点需要进行主从切换来容错。

**Gossip 协议**来广播自己的状态以及自己对整个集群认知的改变。比如A节点发现B节点失联了 (PFail)，它会将这条信息向整个集群广播，其它节点也就可以收到这信息。如果一个节点收到了B节点失联的数量 (PFail Count) 已经达到了**集群的大多数**，就可以标记**该节点为确定下线状态** (Fail)，然后向整个集群广播，强迫其它节点也接收该节点已经下线的事实，并立即对**该失联节点进行主从切换**

#### 基本使用

- 去中心化,由多个节点组成,但是服务连接集群时,有一个节点即可,但是用多个节点更好,因为这个节点挂了，客户端就必须更换地址才

可以继续访问 Cluster

#### 特殊情况,多次重试

前面说的跳转,和迁移过程中的asking都是1次重试操作

那么可能会出现**重试2次**的情况,先A节点收到错误指令,让客户端跳转到B节点,刚好B开始迁移,让客户端askingC节点

也可能出现**多次重试**,Java源码中设置有最大重试次数,超过就报错

#### 集群节点变动通知:

服务器节点变更时，客户端应该即时得到通知以实时刷新自己的节点关系表,怎么得到通知呢?

1. 目标节点挂掉了，客户端会抛出一个 ConnectionError，紧接客户端着会随机挑一个节点来连接，这时新节点会通过**跳转**告知目标槽位被分配到的新的节点地址
2. 运维手动修改了集群信息，将 master 切换到其它节点，并将旧的 master 移除集群。这时打在旧节点上的指令会收到一个 ClusterDown 的错误，告知当前节点所在集群不可用 (当前节点已经被孤立了，它不再属于之前的集群)。这时**客户端就会关闭所有的连接，清空槽位映射关系表**，然后向上层抛错。待下一条指令过来时，就会重新尝试初始化节点信息。

## 拓展

### Stream-多播的消息队列

#### 特性和概念:

- 支持多播
- 消息持久化

数据结构:

<img src="../../../Desktop/assets/image-20230204140334071.png" alt="image-20230204140334071" style="zoom: 50%;" />

每个 Stream 可以挂多个消费组，每个消费组会有个游标 last_delivered_id 在 Stream 数组之上往前移动，表示当前消费组消费到哪条消息。每个消费组都有一个唯一的名称，消费组不会自动创建，需要单独的指令 xgroup create 进行创建，需要指定从 Stream 的某个消息 ID 开始消费，这个 ID 用来初始化 last_delivered_id 变量。

消费组可以挂接多个消费者，消费者之间是**竞争关系**，**任意一个消费者**读取了消息都会使游标 last_delivered_id 往前移动。每个消费者有一个组内唯一名称。

消费者内部会有个**状态变量 pending_ids**，它**记录当前已经被客户端读取的消息**，但是**还没有 ack**。如果客户端没有 ack，这个变量里面的消息 ID 会越来越多，一旦某个消息被 ack，它就开始减少。这个 pending_ids 变量在 Redis 官方被称之为 PEL，这是一个很核心的数据结构，它用来**确保客户端至少消费了消息一次**，而不会在网络传输的中途丢失了没处理

#### 消息id

时间戳-第几条消息,1527846880572-5表示该时间戳下,可以由服务器自动生成，也可以由客户端自己指定，但必须是整数-整数，且后面消息的 ID 要大于前面的消息 ID。

#### 增删改查指令

1. xadd 追加消息
2. xdel 删除消息，这里的删除仅仅是设置了标志位，不影响消息总长度
3. xrange 获取消息列表，会自动过滤已经删除的消息
4. xlen 消息长度
5. del 删除 Stream

#### 独立消费

单独的消费指令 xread,使用 xread 时，我们可以完全忽略消费组 (Consumer Group) 的存在，就好比 Stream 就是一个普通的列表 (list),当 Stream 没有新消息时，甚至可以阻塞等待

```
 xread count 2 streams codehole 0-0//从 Stream 头部读取两条消息
 xread count 1 streams codehole $//从 Stream 尾部读取一条消息
 xread block 0 count 1 streams codehole $//从尾部阻塞等待新消息到来，下面的指令会堵住，直到新消息到来
 其他指令用到的时候自己去查
```

客户端使用 xread 进行顺序消费，一定要记住当前消费到哪里了，也就是返回

的消息 ID。下次继续调用 xread 时把它带上就可以继续消费后续的消息。

block 0 表示永远阻塞，直到消息到来，block 1000 表示阻塞 1s，如果 1s 内没有任何消息到来，就返回 nil。

#### 创建消费组

xgroup create指令创建消费组 ,需要传递起始消息 ID 参数用来初始化 last_delivered_id 变量

```
xgroup create codehole cg1 0-0//示从头开始消费
xgroup create codehole cg2 $//从尾部开始消费，只接受新消息，当前 Stream 消息会全部忽略
xinfo stream codehole//获取 Stream 信息
xinfo groups codehole//获取 Stream 的消费组信息
```

#### 消费

xreadgroup 指令可以进行消费组的**组内消费**,需要提供:消费组名称、消费者名称和起始消息 ID,同 xread 一样，也**可以阻塞等待新消息**

消费者读到新消息后，消息 ID 就会进入 PEL(正在处理的消息) 结构里，客户端处理完毕后使用 xack 指令通知Redis，本条消息已经处理完毕，该消息 ID 就会从 PEL 中移除

```
# > 号表示从当前消费组的 last_delivered_id 后面开始读
# 每当消费者读取一条消息，last_delivered_id 变量就会前进
xreadgroup GROUP cg1 c1 count 1 streams codehole >
xreadgroup GROUP cg1 c1 block 0 count 1 streams codehole >  阻塞等待读取消息
xinfo groups codehole// 观察消费组信息
xinfo consumers codehole cg1//获取cg1消费组内的消费者信息
xack codehole cg1 1527851486781-0//ack 一条消息
```

#### Stream消息太多怎么办

xadd 的指令提供一个定长长度 maxlen，就可以将老的消息干掉，确保最多不超过指定长度

目前**没有根据时间戳删除消息**的方法,xdel只是做了个删除标志

#### 消息如果忘记ACK 会怎样

如果不清理ack,PEL结构占用的内存会变大,如果有很多消费组的话，那么这个 PEL 占用的内存会更大

#### PEL如何避免消息丢失?

客户端消费者读取 Stream 消息后,客户端断开了链接,那么我们的程序可能没收到这个消息.怎么办呢?

 PEL 里已经保存了发出去的消息 ID,客户端重新连上后可以再获取,不过此时 **xreadgroup 的起始消息ID 不能为参数>**，而必须是任意有效的消息 ID，一般将参数设为 0-0，表示读取所有的PEL 消息以及自 last_delivered_id 之后的新消息

#### 分区 

原生不支持分区,想要使用分区分配多个Stream，然后在客户端使用一定的策略来生产消息到不同的 Stream。

月kafka比较:

- kafka原生支持分区,但也是客户端通过hash算法把消息塞入不同分区的
- kafka支持动态增加分区,但是不会对已经存在的数据rehash,很蹩脚,redis通过增加新的stream也可以做到

### Info指令-了解当前redis运行状态

1. Server 服务器运行的环境参数

2. Clients 客户端相关信息

3. Memory 服务器运行内存统计数据

4. Persistence 持久化信息

5. Stats 通用统计数据

6. Replication 主从复制相关信息

7. CPU CPU 使用情况

8. Cluster 集群信息
9. KeySpace 键值对统计数量信息

```
Info 可以一次性获取所有的信息，也可以按块取信息。
# 获取所有信息
> info
# 获取内存相关信息
> info memory
# 获取复制相关信息
> info replication
```

全部细节:https://redis.io/commands/info/

#### 每秒指令多少次?

```
> redis-cli info stats |grep ops   ->所有客户端每秒会发送 789 条指令到服务器执行
instantaneous_ops_per_sec:789
```

Redis 可以每秒执行 10w 次指令,qps 过高,可以考虑通过 monitor 指令快速观察一下究竟是哪些 key 访问比较频繁

```
redis-cli monitor
```

#### 连接了多少客户端?

```
> redis-cli info clients
# Clients
connected_clients:124 # 这个就是正在连接的客户端数量
client_longest_output_list:0
client_biggest_input_buf:0
blocked_clients:0

如果数量不对劲.使用 client list 指令列出所有的客户端链接地址来确定源头

rejected_connections:因为超出最大连接数限制而被拒绝的客户端连接次数,如果数量过大,,表示需要调整最大连接数:maxclients参数
```

#### Redis内存占用多大

```
> redis-cli info memory | grep used | grep human

used_memory_human:827.46K # 内存分配器 (jemalloc) 从操作系统分配的内存总量

used_memory_rss_human:3.61M # 操作系统看到的内存占用 ,top 命令看到的内存

used_memory_peak_human:829.41K # Redis 内存消耗的峰值

used_memory_lua_human:37.00K # lua 脚本引擎占用的内存大小

如果单个 Redis 内存占用过大，并且在业务上没有太多压缩的空间的话，可以考虑集群化了
```

#### 复制积压缓冲区多大

```
> redis-cli info replication |grep backlog
repl_backlog_active:0
repl_backlog_size:1048576 # 这个就是积压缓冲区大小
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
```

这个值非常重要,严重影响到主从复制的效率,主从库断开连接时,，又重新连上的时候，这段时间内发生在master 上的修改操作指令都会放在积压缓冲区中，这样从库可以通过积压缓冲区恢复中断的主从同步过程

积压缓冲区是环形的，**后来的指令会覆盖掉前面的内容**。如果有多个从库复制，积压缓冲区是共享的，它不会因为从库过多而线性增长。如果实例的修改指令请求很频繁，那就把积压缓冲区调大一些，几十个 M 差不多，如果不频繁，就设置为几个 M。

```
> redis-cli info stats | grep sync
sync_full:0
sync_partial_ok:0
sync_partial_err:0 # 半同步失败次数
```

通过查看 **sync_partial_err 变量**的次数来决定是否需要扩大积压缓冲区，它表示**主从半同步复制失败的次数**

### 分布式锁

**问题:**前面讲的分布式锁,单机情况下是比较安全的,但是集群环境下不安全

示例:

Sentinel 集群中，主节点挂掉时，从节点取而代之,原先第一个客户端在主节点中申请成功了一把锁，但是这把锁还没有来得及同步到从节点，主从切换后，这个新的节点内部没有这个锁(也就是前面说的**Sential主从自动切换会导致数据不一致**),此时另一个客户端过来请求加锁时，立即就批准了。这样会导致系统中一把锁被两个客户端同时持有

解决方案:

**Redlock** 算法:虽然比较复杂,但是已有库将其封装

首先需要一些独立的redis实例,采用大多数机制,set和del指令都要超过半数成功,才算成功.

但是需要解决虑**出错重试、时钟漂移**等很多细节问题

**缺点:**

- 需要更多的redis实例,运维成本会上升
- 性能下降,因为加锁解锁要向多个节点读写
- 代码要引入其他库,程序复杂度上升

**适用场景:**

集群环境,且分布式锁要**高可用**,且上锁原因是为了正确性

#### 如何做分布式锁-博客

https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html

**上锁的原因**:

1. 效率:避免重复相同的工作(如高成本的计算)
2. 正确性:防止并发导致的程序混乱,数据错误

如果仅仅是为了效率,那么单机分布式锁一般情况下已经可以满足需求,即使Redis崩溃,带来的成本也仅仅是多执行了几次,浪费了一些资源

**一般如何使用分布式锁:**

**问题:**

**场景**:先获取锁，然后读取文件，进行一些更改，将修改后的文件写回,然后释放锁

首先说到上面的**超时问题**,会出现**更新丢失**的情况,原因可能不单单是程序时间过长导致超时,还可能很多种情况

- GC(垃圾收集器)启动了,GC可以暂停正在运行的线程
- 试图读取资源,比如从磁盘读取数据到内存,发起同步网络请求
- 等等等等

你可能会想,那我再最后的节点进行检查,比如我在写回之前做检查,看看锁是不是过期了,这样也是不行的,线程暂停可以是在任何节点,可能就在你检查好了没问题之后,刚好暂停,超时问题依旧会发生

**解决方案:**

获取锁的时候生成一个单调递增(比如ZooKeeper 作为锁服务，您可以使用`zxid` 或 znode 版本号作为这个值)的值,在最后的写回动作进行校验,如果已经处理了更高值,就拒绝处理本次动作

**结论:**

Redlock 不是个好的选择,如果为保证效率,单机分布式锁也能用,如果为保证正确性,时钟漂移或者上面说的GC等情况都会导致异常

### 过期策略

**问题:**

Redis 是单线程的,处理过期key会不会忙不过来,会不会影响线上指令?

**会过期的key集合**

每个设置了过期时间的 key 放入到一个独立的字典中

- 定时遍历来删除
- 惰性删除,在用到这个key的时候检查有没有过期,过期了就删除

**定时扫描策略**

**默认每秒10次**,不是全部遍历,而是**贪心策略**:

1. 随机选出20个key,删除其中过期的key
2. 如果过期的比例超过四分之一,重新这个过程

为保证不会循环太多次,设置了时间上线,不超过25ms

**问题:**很多key同一时间过期怎么办

1. Redis 会循环多次扫描过期字典,直至稀薄
2. 内存管理器需要频繁回收内存页

这会导致线上读写请求出现明显卡顿,虽然每次最多25ms,但是小卡顿积少成多

解决方案:大批量的key要设置一个随机范围,避免同时过期

#### 从库的过期策略

从库不会过期扫描->主库会在key过期后,AOF文件里增加del指令,从库通过执行这条 del 指令来删除过期的key

由于主从同步时异步的,主库的del语句没有同步到从库的话会导致数据不一致

### LRU

#### 概念和淘汰策略

内存中数据过多,超出内存限制的话,内存数据和磁盘会频繁交换,性能急剧下降,这是不能容忍的,可以通过maxmemory参数设置内存限制

当内存超出时有如下方案来选择怎么腾出空间

- 写请求放弃,读和删除请求可以(默认)
- 尝试淘汰设置了过期时间的 key->最少使用的 key 优先被淘汰
- 尝试淘汰设置了过期时间的 key->key 的剩余寿命 ttl 的值，ttl 越小越优先被淘汰
- 尝试淘汰设置了过期时间的 key->随机淘汰
- 尝试淘汰全体key->最少使用的 key 优先被淘汰
- 尝试淘汰全体key->随机淘汰

只是拿 Redis 做缓存，那应该使用 allkeys-xxx

同时使用 Redis 的持久化功能，那就使用 volatile-xxx 策略

#### LRU算法

双向链表+字典，链表中的元素按照最近被访问的时间进行排列。当空间满的时候，会踢掉链表尾部的元素。当字典的某个元素被访问

时，它在链表中的位置会被移动到表头。

**Redis使用的是近似LRU算法**

使用LRU,Redis要消耗大量内存,并且对享有的数据结构进行改造,因此采用近似LRU算法

在现有数据结构的基础上使用**随机采样法**来淘汰元素，Redis 给每个 key 增加了一个额外的小字段，长度是 24 个 bit:最后一次被访问的时间戳

采用**懒惰处理**,执行写操作时,发现内存超出maxmemory,触发一次淘汰:随机采样出 5(可以配置) 个 key，淘汰掉最旧的 key，如果淘汰后内存还是超出 maxmemory，那就继续随机采样淘汰，直到内存低于maxmemory 为止。

```
maxmemory-policy :上面的方案->allkeys/volatile
maxmemory_samples :每次采样多少个 key,默认是5
```

Redis3.0增加了**淘汰池**:每次淘汰中,淘汰掉最旧的一个 key 之后，保留剩余较旧的 key 列表放入淘汰池中留待下一个循环

### 异步线程

Redis 内部并不是只有一个主线程，它还有几个**异步线程专门用来处理一些耗时的操作**,主线程把指令包装成任务丢进异步队列,后台线程从里面去取,那么也就是说,多个线程会同时操作这个队列,即**任务队列必须线程安全**

- **AOF Sync**操作很耗时也是异步完成,**单独使用一个异步线程**
- 懒惰删除有一个异步线程

#### 懒惰删除

如果删除的 key 是一个非常大的对象，比如一个千万元素的 hash，同步删除会导致单线程卡顿->unlink 指令,丢给后台线程来异步回收内存

```
unlink key->丢给后台线程异步回收,但如果key 所占用的内存很小,会立即回收,即等同于del
flushdb 和 flushall->清空数据库
flushdb async和 flushall async->异步清空数据库
```

#### 更多异步删除点

Redis 回收内存除了 **del 指令**和 **flush** 之外，还会存在于在 **key 的过期**、**LRU 淘汰**、**rename 指令**以及**从库全量同步时接受完 rdb 文件后会立即进行的 flush** 操作。

Redis4.0 为这些删除点也带来了异步删除机制，**打开这些点需要额外的配置选项**。

 1、slave-lazy-flush 从库接受完 rdb 文件后的 flush 操作

 2、lazyfree-lazy-eviction 内存达到 maxmemory 时进行淘汰

 3、lazyfree-lazy-expire key 过期删除

 4、lazyfree-lazy-server-del rename 指令删除 destKey

## 源码

### 字符串

结构如下,类似Java的ArrayList,capacity是容量,len是实际长度

```
struct SDS<T> {
 T capacity; // 数组容量
 T len; // 数组长度
 byte flags; // 特殊标识位，不理睬它
 byte[] content; // 数组内容
}
```

- **字符串可以修改**,因为要支持append(追加)功能,**长度不够长时,要分配数组,原来的内容复制过来**->字符串长度很长时,新数组的内存分配和内容复制开销很大
- 但是正常情况下,不会用append功能,因此**创建字符串时,len和capacity一样长**
- 字符串长度不可以超过512M
- 为什么len和capacity用泛型?->为对内存做极致优化,字符串根据长度len和capacity可以选择用short和byte表示

**embstr vs raw**:

字符串由两种表示方式,短时->emb 形式存储,长度超过44时-> raw 形式存储

问题:两者区别?为什么是44?

#### Redis的对象头结构

**所有的 Redis 对象都有下面的这个结构头**

```
struct RedisObject {
 int4 type; // 4bits ->对象类型
 int4 encoding; // 4bits ->同一个类型的存储形式也可能不同
 int24 lru; // 24bits  记录LRU信息,上面输的近似LRU就是通过这个实现的
 int32 refcount; // 4bytes ->引用计数,为0时被销毁
 void *ptr; // 8bytes，64-bit system->具体指针
} robj;
```

再看字符串比较小时SDS的结构

```
struct SDS {
 int8 capacity; // 1byte
 int8 len; // 1byte
 int8 flags; // 1byte
 byte[] content; // 内联数组，长度为 capacity
}
```

对象头是每个对象都有的,固定占用16字节,SDS是自己的结构,因此一个最小字符串也需要16(对象头)+3(SDS)字节大小

![image-20230206234613707](../../../Desktop/assets/image-20230206234613707.png)

如图所示是**两者存储形式的差别,emb形式分配内存的时候,只需要malloc方法分配一次,raw则是两次**

内存分配时,每次都是2,4,8,16,32,64字节分配,因此为了容纳emb,至少要分配32字节,**如果超过64,就会认定是大字节,就会选用raw**

Redis中字符串是以\0字节结尾(便于直接使用 glibc 的字符串处理函数)的,固定占用一个字节,前面说到**光是结构就占用了19字节**,剩下的用来存字符串内容,就剩下44字节了

#### 扩容

最前面讲过,字符串在长度小于 1M 之前，**扩容空间采用加倍策略**，当长度超过 1M 之后，为了避免加倍后的冗余空间过大而导致浪费，每次扩容只会多分配 **1M 大小的冗余空间**

### 字典



## 压力测试

**管道压力测试**

Redis 自带了一个压力测试工具 redis-benchmark

```
set -> redis-benchmark -t set -q
SET: 51975.05 requests per second  QPS大概是5w
redis-benchmark -t set -P 2 -q  -> -P表示单个管道内并行的请求数量，看下面 P=2，QPS 达到了 9w,P=3，QPS 达到了 10w/s
再继续提升 P 参数，发现 QPS 已经上不去了。这是为什么呢？
因为 CPU 处理能力已经达到了瓶颈，Redis 的单线程 CPU 已经飙到了 100%
```

 

#### 

